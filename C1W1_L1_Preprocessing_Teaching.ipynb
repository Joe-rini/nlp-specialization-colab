{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joe-rini/nlp-specialization-colab/blob/main/C1W1_L1_Preprocessing_Teaching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb0e406-39d8-4aeb-8970-e29d071cb51a",
      "metadata": {
        "id": "7bb0e406-39d8-4aeb-8970-e29d071cb51a"
      },
      "source": [
        "\n",
        "# Lesson L1 ‚Äì Natural Language Preprocessing üåü‚úÇÔ∏èüßπ\n",
        "\n",
        "Welcome to **Lesson 1** of the *NLP Specialization Teaching Edition*.\n",
        "\n",
        "In this hands-on notebook, you‚Äôll learn how to clean raw text to prepare it for both classical ML and modern neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Why does preprocessing matter?\n",
        "Raw text is **messy** ‚Äì full of punctuation, contractions, emojis, and hyperlinks.  \n",
        "Preprocessing helps your model **focus on the signal**:\n",
        "\n",
        "- ‚úÖ Smaller vocabulary  \n",
        "- ‚úÖ More consistent word forms  \n",
        "- ‚úÖ Faster, more accurate models\n",
        "\n",
        "---\n",
        "\n",
        "## üó∫Ô∏è What you‚Äôll do\n",
        "\n",
        "1. Run a **step-by-step walk-through** of one tweet.\n",
        "2. **Explore the full Twitter dataset** used in this course.\n",
        "3. **Wrap a preprocessing pipeline** into a reusable function.\n",
        "4. **Play with a Gradio app** to see it all in action.\n",
        "\n",
        "> üëâ Run each cell, tweak the code, and break things! That‚Äôs the best way to learn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece5b4cc-cef7-478a-a6f3-c1bdd459ddf6",
      "metadata": {
        "id": "ece5b4cc-cef7-478a-a6f3-c1bdd459ddf6"
      },
      "source": [
        "## üçÄ Step 0 ‚Äì Setup and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91b469da-4911-4851-ad55-7a54be1978d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91b469da-4911-4851-ad55-7a54be1978d8",
        "outputId": "53cd0e92-6f54-4df3-eb52-9a67e6a42427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.2 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.1.post1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.25.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ Setup complete\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q --upgrade numpy==1.26.4 scikit-learn==1.4.1.post1 nltk==3.8.1 gradio==4.27.0 --progress-bar off\n",
        "\n",
        "import nltk, re, string, random\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK resources\n",
        "for res in ['stopwords', 'twitter_samples', 'punkt']:\n",
        "    nltk.download(res, quiet=True)\n",
        "\n",
        "print(\"‚úÖ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01aa24e-d234-4bf6-9405-1b9d5d13fc55",
      "metadata": {
        "id": "c01aa24e-d234-4bf6-9405-1b9d5d13fc55"
      },
      "source": [
        "## 1Ô∏è‚É£ Toy example ‚Äì step-by-step preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4b4915-bdc4-480a-a5bd-85bb12b721a4",
      "metadata": {
        "id": "8a4b4915-bdc4-480a-a5bd-85bb12b721a4"
      },
      "source": [
        "\n",
        "Let‚Äôs warm up with one short sentence and manually walk through:\n",
        "\n",
        "1. **Tokenization**\n",
        "2. **Stop‚Äëword and punctuation removal**\n",
        "3. **Stemming**\n",
        "\n",
        "So you can *see* the effect of each transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aa83bea0-02c0-46e3-9fb1-72b86e6d7f16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa83bea0-02c0-46e3-9fb1-72b86e6d7f16",
        "outputId": "6e503ac7-9a16-470e-cdd6-996ce8c7bc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî∏ Original sentence:\n",
            "I love Natural Language Processing! :) #NLP\n",
            "\n",
            "üîπ After tokenization:\n",
            "['i', 'love', 'natural', 'language', 'processing', '!', ':)', '#nlp']\n",
            "\n",
            "üîπ After stop-word & punctuation filtering:\n",
            "['love', 'natural', 'language', 'processing', ':)', '#nlp']\n",
            "\n",
            "üîπ After stemming:\n",
            "['love', 'natur', 'languag', 'process', ':)', '#nlp']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sentence = \"I love Natural Language Processing! :) #NLP\"\n",
        "\n",
        "print(\"üî∏ Original sentence:\")\n",
        "print(sentence)\n",
        "\n",
        "# 1. Tokenization\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"\\nüîπ After tokenization:\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Stop-word & punctuation removal\n",
        "stopwords_en = stopwords.words('english')\n",
        "tokens_no_sw = [w for w in tokens if w not in stopwords_en and w not in string.punctuation]\n",
        "print(\"\\nüîπ After stop-word & punctuation filtering:\")\n",
        "print(tokens_no_sw)\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in tokens_no_sw]\n",
        "print(\"\\nüîπ After stemming:\")\n",
        "print(stemmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f24c50-8af4-4c63-9e3d-a36cb2160f0a",
      "metadata": {
        "id": "a1f24c50-8af4-4c63-9e3d-a36cb2160f0a"
      },
      "source": [
        "\n",
        "**What‚Äôs happening?**\n",
        "\n",
        "- The tokenizer converts the sentence to lower-case and breaks it into tokens.\n",
        "- Stop-words like \"I\" are removed ‚Äì they carry little meaning in sentiment tasks.\n",
        "- Stemming reduces words to their roots (*processing ‚Üí process*), shrinking your model's vocabulary.\n",
        "\n",
        "This is the backbone of many real NLP pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d450553-dac3-4cd7-b74e-e1c0bec41216",
      "metadata": {
        "id": "9d450553-dac3-4cd7-b74e-e1c0bec41216"
      },
      "source": [
        "## 2Ô∏è‚É£ Peek at the Twitter dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae35aed7-bfd5-49cd-b435-c83698c3e994",
      "metadata": {
        "id": "ae35aed7-bfd5-49cd-b435-c83698c3e994"
      },
      "source": [
        "\n",
        "The NLTK `twitter_samples` corpus contains:\n",
        "\n",
        "- **5,000 positive tweets**  \n",
        "- **5,000 negative tweets**\n",
        "\n",
        "Let‚Äôs load and preview it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b770a6a2-9b69-4c6a-b96e-3b6b4dfef235",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b770a6a2-9b69-4c6a-b96e-3b6b4dfef235",
        "outputId": "3b73d3ab-235d-4c9d-d39e-cbaad038fad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 5,000 positive tweets\n",
            "‚úÖ Loaded 5,000 negative tweets\n",
            "\n",
            "üî∏ Example positive tweet:\n",
            "@miccoelaine @KRMN4L nice dave :D\n",
            "\n",
            "üî∏ Example negative tweet:\n",
            "@achiralk thanks for the feedback. Here I was thinking that perhaps Mobitel may be better off, heck, guess not :(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(all_positive_tweets):,} positive tweets\")\n",
        "print(f\"‚úÖ Loaded {len(all_negative_tweets):,} negative tweets\")\n",
        "\n",
        "print(\"\\nüî∏ Example positive tweet:\")\n",
        "print(random.choice(all_positive_tweets))\n",
        "\n",
        "print(\"\\nüî∏ Example negative tweet:\")\n",
        "print(random.choice(all_negative_tweets))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea89d9c7-7865-4aa3-9444-6d7e022ec1bf",
      "metadata": {
        "id": "ea89d9c7-7865-4aa3-9444-6d7e022ec1bf"
      },
      "source": [
        "## 3Ô∏è‚É£ Wrap it up: a reusable `process_tweet()` function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b41f51-25ec-412d-a1c5-7096aae7a18f",
      "metadata": {
        "id": "e7b41f51-25ec-412d-a1c5-7096aae7a18f"
      },
      "source": [
        "\n",
        "We‚Äôll combine the steps into a helper function you can reuse later.\n",
        "\n",
        "This includes:\n",
        "- Lowercasing  \n",
        "- Removing links, handles, and hashtags  \n",
        "- Tokenizing  \n",
        "- Filtering  \n",
        "- Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2fd62603-2524-480f-92b5-a6e250f75140",
      "metadata": {
        "id": "2fd62603-2524-480f-92b5-a6e250f75140"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_tweet(tweet: str):\n",
        "    \"\"\"Preprocess a single tweet into cleaned, stemmed tokens.\"\"\"\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)  # remove links\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)                     # remove @handles\n",
        "    tweet = re.sub(r'#', '', tweet)                         # strip hashtags\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    stopwords_en = stopwords.words('english')\n",
        "    tokens_clean = [tok for tok in tokens if tok not in stopwords_en and tok not in string.punctuation]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(tok) for tok in tokens_clean]\n",
        "\n",
        "    return stems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "78e72e0a-d1d9-4e41-80d7-90ba0d9e09c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e72e0a-d1d9-4e41-80d7-90ba0d9e09c5",
        "outputId": "abba24b9-ac0e-4cfc-da16-74522972e5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tweet:\n",
            " My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off‚Ä¶ https://t.co/3tfYom0N1i\n",
            "\n",
            "Processed tokens:\n",
            "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '‚Ä¶']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample = all_positive_tweets[2277]\n",
        "print(\"Original tweet:\\n\", sample)\n",
        "\n",
        "print(\"\\nProcessed tokens:\")\n",
        "print(process_tweet(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629ee9dd",
      "metadata": {
        "id": "629ee9dd"
      },
      "source": [
        "‚ú® *Much cleaner!* You can now use these tokens in classifiers or embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b582f7b-d8ac-492d-9864-075a50c0c8cb",
      "metadata": {
        "id": "6b582f7b-d8ac-492d-9864-075a50c0c8cb"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ You finished Lesson L1!\n",
        "\n",
        "You're now equipped to:\n",
        "- Clean and tokenize tweets for downstream tasks\n",
        "- Understand the role of stop-word filtering and stemming\n",
        "- Build preprocessing pipelines for larger models\n",
        "\n",
        "üëâ In the next lesson, we‚Äôll use these tokens to build features and train your first classifier!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}