{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713dc8f3",
   "metadata": {},
   "source": [
    "# Assignment‚ÄØA2¬†‚Äì¬†Na√Øve‚ÄØBayes from Scratch¬†üîßü™Ñ\n",
    "This notebook is **autograder‚Äëcompatible** with the Coursera version. All required function names and signatures match.\n",
    "\n",
    "### How to use\n",
    "* Work through the TODO cells labelled `### UNQ_Cx`.\n",
    "* Run the hidden **local sanity checks** ‚Äì when they pass, you should pass Coursera.\n",
    "* At the end you‚Äôll find alias functions that map to the exact autograder names plus a commented `grader.check_all()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a066b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçÄ Setup ‚Äì pinned versions to avoid Colab conflicts\n",
    "!pip -q install --upgrade nltk numpy>=1.26,<2.1 gradio>=4.27.0 websockets>=13,<15 --progress-bar off\n",
    "import nltk, ssl, warnings; warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "for res in ['stopwords','punkt','twitter_samples']:\n",
    "    nltk.download(res, quiet=True)\n",
    "print('‚úÖ Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c1293",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£¬†Helper functions (provided, no edits needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd38f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re, random, math\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Tokenise, normalise, remove stop‚Äëwords & stem.\"\"\"\n",
    "    tweet = tweet.lower(); tweet = re.sub(r'https?://\\S+','',tweet)\n",
    "    tweet = re.sub(r'[^a-z\\s]','',tweet)\n",
    "    return [stemmer.stem(w) for w in tweet.split() if w not in stop_words]\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    \"\"\"Populate result dict with frequency of (word, label).\"\"\"\n",
    "    for y,t in zip(ys, tweets):\n",
    "        for w in process_tweet(t):\n",
    "            pair=(w,y)\n",
    "            result[pair]=result.get(pair,0)+1\n",
    "    return result\n",
    "\n",
    "def lookup(freqs, word, label):\n",
    "    return freqs.get((word,label),0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c02cfd",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£¬†Load data & split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = twitter_samples.strings('positive_tweets.json')\n",
    "neg = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = pos + neg; ys = np.array([1]*len(pos)+[0]*len(neg))\n",
    "random.seed(0); idx=list(range(len(tweets))); random.shuffle(idx)\n",
    "tweets=[tweets[i] for i in idx]; ys=ys[idx]\n",
    "split=int(0.8*len(tweets))\n",
    "tweets_tr,tweets_te = tweets[:split],tweets[split:]\n",
    "ys_tr,ys_te = ys[:split],ys[split:]\n",
    "print(len(tweets_tr),'train,',len(tweets_te),'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af874ea",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£¬†Implement Na√Øve‚ÄØBayes\n",
    "Fill the two functions below. Follow the Coursera autograder signature exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728229b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNQ_C1\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    \"\"\"Returns logprior, loglikelihood dict.\"\"\"\n",
    "    ###¬†START CODE HERE¬†###\n",
    "    loglikelihood = {}\n",
    "    vocab = {w for (w,_) in freqs.keys()}\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for pair,c in freqs.items():\n",
    "        if pair[1]==1:\n",
    "            N_pos += c\n",
    "        else:\n",
    "            N_neg += c\n",
    "    D = len(train_y)\n",
    "    D_pos = (train_y==1).sum(); D_neg = (train_y==0).sum()\n",
    "    logprior = math.log(D_pos) - math.log(D_neg)\n",
    "    for w in vocab:\n",
    "        f_pos = freqs.get((w,1),0)\n",
    "        f_neg = freqs.get((w,0),0)\n",
    "        p_w_pos = (f_pos+1)/(N_pos+V)\n",
    "        p_w_neg = (f_neg+1)/(N_neg+V)\n",
    "        loglikelihood[w] = math.log(p_w_pos/p_w_neg)\n",
    "    ###¬†END CODE HERE¬†###\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNQ_C2\n",
    "def predict_sentiment(tweet, logprior, loglikelihood):\n",
    "    ###¬†START CODE HERE¬†###\n",
    "    words = process_tweet(tweet)\n",
    "    score = logprior\n",
    "    for w in words:\n",
    "        if w in loglikelihood:\n",
    "            score += loglikelihood[w]\n",
    "    return 1 if score>0 else 0\n",
    "    ###¬†END CODE HERE¬†###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNQ_C3\n",
    "def get_ratio(freqs, word):\n",
    "    pos = freqs.get((word,1),0)\n",
    "    neg = freqs.get((word,0),0)\n",
    "    return (pos+1)/(neg+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5eae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNQ_C4\n",
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    out = {}\n",
    "    for w in {w for w,_ in freqs.keys()}:\n",
    "        ratio = get_ratio(freqs, w)\n",
    "        if label==1 and ratio>=threshold:\n",
    "            out[w]=ratio\n",
    "        elif label==0 and ratio<=1/threshold:\n",
    "            out[w]=ratio\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12489f92",
   "metadata": {},
   "source": [
    "### Train model with your functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa768dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, tweets_tr, ys_tr)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, tweets_tr, ys_tr)\n",
    "y_hat = [predict_sentiment(t, logprior, loglikelihood) for t in tweets_te]\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('Test accuracy', accuracy_score(ys_te, y_hat))\n",
    "print(classification_report(ys_te, y_hat, target_names=['neg','pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f830488",
   "metadata": {},
   "source": [
    "### Local sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predict_sentiment(\"I love it\", logprior, loglikelihood) in [0,1]\n",
    "assert abs(get_ratio(freqs,'love') - ((freqs.get(('love',1),0)+1)/(freqs.get(('love',0),0)+1)))<1e-9\n",
    "print('Local checks passed ‚úîÔ∏è')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49391aad",
   "metadata": {},
   "source": [
    "### Function aliases (for Coursera grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already using Coursera names ‚Äì included for completeness\n",
    "# count_tweets, train_naive_bayes, predict_sentiment, get_ratio, get_words_by_threshold already defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506b09d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£¬†Gradio tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f175e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr, numpy as np, math\n",
    "def classify(text):\n",
    "    words=process_tweet(text)\n",
    "    score=logprior+sum(loglikelihood.get(w,0) for w in words)\n",
    "    prob=1/(1+math.exp(-score))\n",
    "    return {'Prob‚Äëpositive': round(prob,3), 'Prediction': 'Positive üòä' if prob>=0.5 else 'Negative üòû'}\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('### üîç Na√Øve‚ÄØBayes sentiment tester')\n",
    "    txt=gr.Textbox(lines=3); out=gr.JSON()\n",
    "    txt.submit(classify, txt, out); gr.Button('Run').click(classify, txt, out)\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5bac8",
   "metadata": {},
   "source": [
    "---\n",
    "When you‚Äôre happy, copy your `UNQ_Cx` cell code into the Coursera assignment and uncomment the grader line:\n",
    "```python\n",
    "# from grader import grader\n",
    "# grader.check_all()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
