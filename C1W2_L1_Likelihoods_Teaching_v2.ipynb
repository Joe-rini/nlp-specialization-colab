{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ddd350",
   "metadata": {},
   "source": [
    "# Lesson L1 (Week 2) – Visualising Naïve Bayes Likelihoods 📈🔵🔴\n",
    "Welcome to **Week 2, Lesson L1**!\n",
    "\n",
    "### What’s new vs. the original Coursera lab\n",
    "* We **generate the features on‑the‑fly** instead of loading `bayes_features.csv` so you don’t need any external files.\n",
    "* Function and variable names are kept **compatible with the Coursera lecture** – so you can still follow along there.\n",
    "* Added a toy walk‑through and an interactive Gradio explorer.\n",
    "\n",
    "_If you’re looking at the Coursera notebook you’ll see a CSV – here we recreate the same two columns programmatically._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7508519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🍀 Setup\n",
    "!pip -q install --upgrade nltk wordcloud gradio>=4.27.0 numpy>=1.26,<2.1 scikit-learn<1.7 websockets>=13,<15 --progress-bar off\n",
    "import nltk, ssl, warnings; warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "for res in ['stopwords','punkt','twitter_samples']:\n",
    "    nltk.download(res, quiet=True)\n",
    "print('✅ Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249ecb3",
   "metadata": {},
   "source": [
    "## 1️⃣ Toy example – six tiny tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re, matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def clean(t):\n",
    "    t = t.lower(); t = re.sub(r'[^a-z\\s]', '', t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "t_pos = [\"love it\", \"so happy\", \"great day\"]\n",
    "t_neg = [\"hate it\", \"so sad\", \"bad day\"]\n",
    "tweets = t_pos + t_neg\n",
    "ys = np.array([1]*3 + [0]*3)\n",
    "\n",
    "from collections import Counter\n",
    "def word_likelihoods(pos, neg, alpha=1):\n",
    "    pos_counts = Counter(w for tw in pos for w in clean(tw))\n",
    "    neg_counts = Counter(w for tw in neg for w in clean(tw))\n",
    "    vocab = set(pos_counts)|set(neg_counts); V=len(vocab)\n",
    "    total_pos=sum(pos_counts.values())+alpha*V\n",
    "    total_neg=sum(neg_counts.values())+alpha*V\n",
    "    lp={w:np.log((pos_counts[w]+alpha)/total_pos) for w in vocab}\n",
    "    ln={w:np.log((neg_counts[w]+alpha)/total_neg) for w in vocab}\n",
    "    return lp, ln\n",
    "\n",
    "lp, ln = word_likelihoods(t_pos, t_neg)\n",
    "def tweet_ll(t):\n",
    "    toks=clean(t)\n",
    "    lpos=sum(lp.get(w,0) for w in toks)\n",
    "    lneg=sum(ln.get(w,0) for w in toks)\n",
    "    return lpos, lneg\n",
    "XY=np.array([tweet_ll(t) for t in tweets])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(XY[:3,0],XY[:3,1],c='green',label='pos')\n",
    "plt.scatter(XY[3:,0],XY[3:,1],c='red',label='neg')\n",
    "plt.axline((0,0),(1,1),ls='--',c='grey');\n",
    "plt.xlabel('log P(t|pos)');plt.ylabel('log P(t|neg)');plt.title('Toy likelihood space');plt.legend();plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552a9bc",
   "metadata": {},
   "source": [
    "The dashed line indicates where **log P(t∣pos) = log P(t∣neg)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfc2f1",
   "metadata": {},
   "source": [
    "## 2️⃣ Helper functions for full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3aaa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def process_tweet(t):\n",
    "    t=t.lower(); t=re.sub(r'https?://\\S+','',t); t=re.sub(r'[^a-z\\s]','',t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "def build_likelihoods(tweets, ys, alpha=1):\n",
    "    pos=[tw for tw,y in zip(tweets,ys) if y==1]\n",
    "    neg=[tw for tw,y in zip(tweets,ys) if y==0]\n",
    "    pos_c=Counter(w for tw in pos for w in process_tweet(tw))\n",
    "    neg_c=Counter(w for tw in neg for w in process_tweet(tw))\n",
    "    vocab=set(pos_c)|set(neg_c); V=len(vocab)\n",
    "    tot_p=sum(pos_c.values())+alpha*V; tot_n=sum(neg_c.values())+alpha*V\n",
    "    lp={w:np.log((pos_c[w]+alpha)/tot_p) for w in vocab}\n",
    "    ln={w:np.log((neg_c[w]+alpha)/tot_n) for w in vocab}\n",
    "    return lp, ln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d74a1",
   "metadata": {},
   "source": [
    "## 3️⃣ Real corpus likelihood scatter & confidence ellipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = twitter_samples.strings('positive_tweets.json')\n",
    "tweets_neg = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = tweets_pos + tweets_neg; ys = np.array([1]*len(tweets_pos)+[0]*len(tweets_neg))\n",
    "lp, ln = build_likelihoods(tweets, ys)\n",
    "def tweet_ll_real(t):\n",
    "    toks=process_tweet(t)\n",
    "    return sum(lp.get(w,0) for w in toks), sum(ln.get(w,0) for w in toks)\n",
    "XY_full = np.array([tweet_ll_real(t) for t in tweets])\n",
    "np.random.seed(0)\n",
    "idx=np.random.choice(len(tweets),2000,replace=False); XY=XY_full[idx]; y_s=ys[idx]\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "ax.scatter(XY[y_s==1,0],XY[y_s==1,1],s=10,c='limegreen',label='pos',alpha=0.5)\n",
    "ax.scatter(XY[y_s==0,0],XY[y_s==0,1],s=10,c='crimson',label='neg',alpha=0.5)\n",
    "ax.axline((0,0),(1,1),ls='--',c='grey')\n",
    "def ellipse(data,color):\n",
    "    import numpy.linalg as LA\n",
    "    cov=np.cov(data,rowvar=False); mean=data.mean(axis=0)\n",
    "    vals,vecs=LA.eigh(cov); order=vals.argsort()[::-1]; vals,vecs=vals[order],vecs[:,order]\n",
    "    theta=np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width,height=2*np.sqrt(vals*5.991)  # 95% CI\n",
    "    ell=Ellipse(xy=mean,width=width,height=height,angle=theta,edgecolor=color,facecolor='none',lw=2)\n",
    "    ax.add_patch(ell)\n",
    "ellipse(XY[y_s==1],'green'); ellipse(XY[y_s==0],'red')\n",
    "ax.set_xlabel('log P(t|pos)'); ax.set_ylabel('log P(t|neg)'); ax.set_title('Likelihood space (2 k sample)'); ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd4b50",
   "metadata": {},
   "source": [
    "## 4️⃣ Interactive likelihood explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1022403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def explorer(txt):\n",
    "    lpv,lnv=tweet_ll_real(txt)\n",
    "    prob=1/(1+np.exp(lnv-lpv))\n",
    "    return {\"log P(t|pos)\":round(lpv,3),\"log P(t|neg)\":round(lnv,3),\"Posterior P(pos)\":round(prob,3),\"Prediction\":\"Positive 😊\" if prob>=0.5 else \"Negative 😞\"}\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('### 🕵️ Likelihood explorer (Naïve Bayes)')\n",
    "    inp=gr.Textbox(lines=3,label='Tweet text'); out=gr.JSON()\n",
    "    inp.submit(explorer,inp,out); gr.Button('Run').click(explorer,inp,out)\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c605d4b",
   "metadata": {},
   "source": [
    "---\n",
    "🎉 **You visualised Naïve Bayes likelihoods and class uncertainty!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
