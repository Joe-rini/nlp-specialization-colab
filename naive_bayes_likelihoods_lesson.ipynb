{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01b39fd-4b20-468d-88a4-8d60080fd770",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson L1 (Week 2) ‚Äì Visualising Naive Bayes Likelihoods üìàüîµüî¥\n",
    "\n",
    "Welcome to **Week 2, Lesson L1** of the *Practical NLP for Beginners* mini‚Äëcourse!\n",
    "\n",
    "### üë©üèª‚Äçüè´ What you‚Äôll learn in the next 30 minutes\n",
    "* **Recap the Naive Bayes classifier** and why log‚Äëlikelihoods are handy.\n",
    "* **Generate features on‚Äëthe‚Äëfly** ‚Äî no external CSV needed.\n",
    "* **Plot tweets in likelihood space** and see the decision boundary appear.\n",
    "* **Interpret confidence ellipses** around positive vs. negative sentiment.\n",
    "* **Play with an interactive explorer** and watch probabilities update live.\n",
    "\n",
    "> **Prerequisites:** only basic Python and NumPy.  \n",
    "> **Goal:** gain an *intuitive* feel for how Naive Bayes classifies text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b173a4b-70b9-46ad-a5ca-4330cc3f21bd",
   "metadata": {},
   "source": [
    "\n",
    "---  \n",
    "### üÜï What‚Äôs different from the original Coursera lab?\n",
    "| Change | Why it matters |\n",
    "|--------|----------------|\n",
    "| **Features generated in‚Äënotebook** | Everything is reproducible ‚Äî no `bayes_features.csv` download. |\n",
    "| **Coursera‚Äëcompatible names** | You can still follow Andrew‚Äôs videos line‚Äëby‚Äëline. |\n",
    "| **Tiny toy walk‚Äëthrough** | The maths becomes tangible before we scale up. |\n",
    "| **Interactive Gradio explorer** | Probe single tweets and *see* the numbers change. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6671115-7ec1-4789-9520-ec47ba1ebd3d",
   "metadata": {},
   "source": [
    "## üçÄ¬†0 ‚Äì Environment set‚Äëup (‚âà¬†1‚ÄØmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de63ad5-2678-42df-bf54-f014d162c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçÄ¬†Setup¬†‚Äì installs take ~45‚ÄØs ‚è≥\n",
    "!pip -q install --upgrade nltk wordcloud gradio>=4.27.0 numpy>=1.26,<2.1 scikit-learn<1.7 websockets>=13,<15 --progress-bar off\n",
    "\n",
    "import nltk, ssl, warnings; warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "for res in ['stopwords', 'punkt', 'twitter_samples']:\n",
    "    nltk.download(res, quiet=True)\n",
    "print('‚úÖ¬†Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fe063-34ae-45f2-8383-38b6cb67fa42",
   "metadata": {},
   "source": [
    "\n",
    "That single cell:\n",
    "\n",
    "1. Installs the **only external libraries** we need.  \n",
    "2. Downloads three corpora from *NLTK*.  \n",
    "3. Prints a green check ‚úîÔ∏è when done.\n",
    "\n",
    "> **Tip:** if the download fails once, just rerun ‚Äî it is cached after the first success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7617267-349d-4924-b021-bf1083bb4102",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£¬†Toy example ‚Äì six tiny tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21cbdb-814d-4a66-9df2-13561dff2c82",
   "metadata": {},
   "source": [
    "\n",
    "We warm up with *six* two‚Äëword tweets:\n",
    "\n",
    "```text\n",
    "Positive ‚Üí  love it | so happy | great day\n",
    "Negative ‚Üí  hate it | so sad   | bad day\n",
    "```\n",
    "\n",
    "A microscopic dataset means we can **calculate every probability by hand** and *see* how Laplace smoothing nudges them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b4d2c-c0c7-4a9c-b62d-3d3357b171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re, matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def clean(t: str):\n",
    "    \"\"\"Lower‚Äëcase ‚Üí remove non‚Äëletters ‚Üí tokenize ‚Üí stop‚Äëword filter ‚Üí stem\"\"\"\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'[^a-z\\s]', '', t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "# Tiny corpus\n",
    "t_pos = [\"love it\", \"so happy\", \"great day\"]\n",
    "t_neg = [\"hate it\", \"so sad\", \"bad day\"]\n",
    "tweets = t_pos + t_neg\n",
    "ys = np.array([1]*3 + [0]*3)  # 1¬†=¬†pos,¬†0¬†=¬†neg\n",
    "\n",
    "# Likelihood helpers --------------------------------------------------------\n",
    "from collections import Counter\n",
    "def word_likelihoods(pos, neg, alpha=1):\n",
    "    pos_counts = Counter(w for tw in pos for w in clean(tw))\n",
    "    neg_counts = Counter(w for tw in neg for w in clean(tw))\n",
    "    vocab = set(pos_counts)|set(neg_counts); V = len(vocab)\n",
    "    tot_pos = sum(pos_counts.values()) + alpha*V\n",
    "    tot_neg = sum(neg_counts.values()) + alpha*V\n",
    "    lp = {w: np.log((pos_counts[w]+alpha)/tot_pos) for w in vocab}\n",
    "    ln = {w: np.log((neg_counts[w]+alpha)/tot_neg) for w in vocab}\n",
    "    return lp, ln\n",
    "\n",
    "lp, ln = word_likelihoods(t_pos, t_neg)\n",
    "\n",
    "def tweet_ll(t):\n",
    "    toks = clean(t)\n",
    "    lpos = sum(lp.get(w, 0) for w in toks)\n",
    "    lneg = sum(ln.get(w, 0) for w in toks)\n",
    "    return lpos, lneg\n",
    "\n",
    "XY = np.array([tweet_ll(t) for t in tweets])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(XY[:3,0], XY[:3,1], c='green', label='pos')\n",
    "plt.scatter(XY[3:,0], XY[3:,1], c='red', label='neg')\n",
    "plt.axline((0,0), (1,1), ls='--', c='grey')\n",
    "plt.xlabel('log P(t|pos)'); plt.ylabel('log P(t|neg)')\n",
    "plt.title('Tiny‚Äëtweet likelihood space'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d716c9-bfff-406f-9656-9a61531db22e",
   "metadata": {},
   "source": [
    "\n",
    "üé® **Reading the picture**\n",
    "\n",
    "* Each dot is a **tweet** embedded in 2‚ÄëD: \\((\\log P(t\\mid\\text{pos}),\\log P(t\\mid\\text{neg}))\\).\n",
    "* The **grey dashed line** is where the two logs are equal ‚Äî our *decision boundary* when priors are 50 / 50.\n",
    "* Points below the line have larger positive likelihood ‚Üí classified *positive*, and vice‚Äëversa.\n",
    "\n",
    "> **Play:** change `alpha` in `word_likelihoods` or modify a tweet, rerun, and watch the dots move!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c229059-bdeb-4e45-ac92-60f2a6555b4c",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£¬†Helper functions for the full corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740e36e-89b1-4c11-a0e1-1fac2820ded8",
   "metadata": {},
   "source": [
    "\n",
    "The next cell builds two helpers:\n",
    "\n",
    "1. **`process_tweet`** ‚Äì lower‚Äëcases, strips URLs, removes punctuation & stop‚Äëwords, then stems.  \n",
    "2. **`build_likelihoods`** ‚Äì computes smoothed log‚Äëlikelihood dictionaries for *all* words.\n",
    "\n",
    "Feel free to tweak the pre‚Äëprocessing or smoothing factor *Œ±*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7006abd-d60a-4705-97f1-670e07f5dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def process_tweet(t: str):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'https?://\\S+', '', t)\n",
    "    t = re.sub(r'[^a-z\\s]', '', t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "def build_likelihoods(tweets, ys, alpha=1):\n",
    "    pos = [tw for tw, y in zip(tweets, ys) if y == 1]\n",
    "    neg = [tw for tw, y in zip(tweets, ys) if y == 0]\n",
    "    pos_c = Counter(w for tw in pos for w in process_tweet(tw))\n",
    "    neg_c = Counter(w for tw in neg for w in process_tweet(tw))\n",
    "    vocab = set(pos_c)|set(neg_c); V = len(vocab)\n",
    "    tot_p = sum(pos_c.values()) + alpha*V\n",
    "    tot_n = sum(neg_c.values()) + alpha*V\n",
    "    lp = {w: np.log((pos_c[w]+alpha)/tot_p) for w in vocab}\n",
    "    ln = {w: np.log((neg_c[w]+alpha)/tot_n) for w in vocab}\n",
    "    return lp, ln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c11bd-e71c-4658-a3a0-1a1f0d717728",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£¬†Real‚Äëcorpus likelihood scatter & confidence ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792d563-3728-491c-a0ee-41092f20e3db",
   "metadata": {},
   "source": [
    "\n",
    "The NLTK Twitter corpus has 5 000 positive and 5 000 negative tweets labelled by crowd‚Äëworkers.\n",
    "\n",
    "We‚Äôll plot a **random sample of 2 000** and draw **95 % confidence ellipses** so the class shapes are still visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725df04-90cd-4f41-a5eb-d491fed28f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = twitter_samples.strings('positive_tweets.json')\n",
    "tweets_neg = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = tweets_pos + tweets_neg\n",
    "ys = np.array([1]*len(tweets_pos) + [0]*len(tweets_neg))\n",
    "\n",
    "lp, ln = build_likelihoods(tweets, ys)\n",
    "\n",
    "def tweet_ll_real(t):\n",
    "    toks = process_tweet(t)\n",
    "    return sum(lp.get(w,0) for w in toks), sum(ln.get(w,0) for w in toks)\n",
    "\n",
    "XY_full = np.array([tweet_ll_real(t) for t in tweets])\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(len(tweets), 2000, replace=False)\n",
    "XY = XY_full[idx]; y_s = ys[idx]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(XY[y_s==1,0], XY[y_s==1,1], s=10, c='limegreen', label='pos', alpha=0.5)\n",
    "ax.scatter(XY[y_s==0,0], XY[y_s==0,1], s=10, c='crimson', label='neg', alpha=0.5)\n",
    "ax.axline((0,0), (1,1), ls='--', c='grey')\n",
    "\n",
    "def ellipse(data, color):\n",
    "    import numpy.linalg as LA\n",
    "    cov = np.cov(data, rowvar=False); mean = data.mean(axis=0)\n",
    "    vals, vecs = LA.eigh(cov); order = vals.argsort()[::-1]\n",
    "    vals, vecs = vals[order], vecs[:,order]\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width, height = 2*np.sqrt(vals*5.991)  # œá2 0.95, df=2\n",
    "    ell = Ellipse(mean, width, height, theta, edgecolor=color,\n",
    "                  facecolor='none', lw=2)\n",
    "    ax.add_patch(ell)\n",
    "\n",
    "ellipse(XY[y_s==1], 'green'); ellipse(XY[y_s==0], 'red')\n",
    "ax.set_xlabel('log P(t|pos)'); ax.set_ylabel('log P(t|neg)')\n",
    "ax.set_title('Likelihood space ‚Äì 2‚ÄØ000‚Äëtweet sample'); ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e228eb-070e-4f90-9f6f-5b48f6369863",
   "metadata": {},
   "source": [
    "\n",
    "üîç **Interpreting the ellipses**\n",
    "\n",
    "* They enclose approximately 95 % of tweets *if* the class distributions were Gaussian.  \n",
    "* **Tilt** ‚Üí correlation between the two log‚Äëlikelihoods.  \n",
    "* **Area** ‚Üí overall variance (here, negative tweets are slightly more spread).\n",
    "\n",
    "> **Exercise:** bump the sample size from 2 000 to 10 000 (all tweets). Does the plot look more or less Gaussian?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3865eaa-be03-429f-962f-bd9f5e649ad3",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£¬†Interactive likelihood explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3350386-b455-4256-8e71-8361b614ab5d",
   "metadata": {},
   "source": [
    "\n",
    "Type *any* sentence below and watch the numbers appear.\n",
    "\n",
    "| Field | Meaning |\n",
    "|-------|---------|\n",
    "| **log P(t\\|pos)** | Sum of per‚Äëword logs under the positive model |\n",
    "| **log P(t\\|neg)** | Same, but negative |\n",
    "| **Posterior P(pos)** | Assuming equal class priors |\n",
    "| **Prediction** | Emoji‚Äëlevel TL;DR |\n",
    "\n",
    "Try:\n",
    "\n",
    "* ‚ÄúI absolutely adore this!‚Äù ‚Üí should go green.  \n",
    "* ‚ÄúThis is the worst thing ever.‚Äù ‚Üí likely red.  \n",
    "* ‚ÄúNothing special, meh.‚Äù ‚Üí often near 0.5, showing model uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700e6fb-f41a-4f23-8f01-2367c9cc1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def explorer(txt):\n",
    "    lpv, lnv = tweet_ll_real(txt)\n",
    "    prob = 1 / (1 + np.exp(lnv - lpv))\n",
    "    return {\n",
    "        \"log P(t|pos)\": round(lpv, 3),\n",
    "        \"log P(t|neg)\": round(lnv, 3),\n",
    "        \"Posterior P(pos)\": round(prob, 3),\n",
    "        \"Prediction\": \"Positive üòä\" if prob >= 0.5 else \"Negative üòû\"\n",
    "    }\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('### üïµÔ∏è¬†Likelihood explorer (Naive¬†Bayes)')\n",
    "    inp = gr.Textbox(lines=3, label='Tweet text')\n",
    "    out = gr.JSON()\n",
    "    inp.submit(explorer, inp, out)\n",
    "    gr.Button('Run').click(explorer, inp, out)\n",
    "\n",
    "# Uncomment when running locally or on Colab:\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c8dd0-76d2-461a-b876-cbd4043e186e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You visualised Naive Bayes likelihoods, interpreted class uncertainty, and built an interactive demo‚Äî*all in one notebook*.\n",
    "\n",
    "### ‚úèÔ∏è Mini‚Äëproject\n",
    "1. Replace the Twitter corpus with **your own data** (e.g. IMDB reviews).  \n",
    "2. Re‚Äëtrain `build_likelihoods`.  \n",
    "3. Plot the new space ‚Äî does the separation get cleaner or messier?  \n",
    "4. Share a screenshot with the community!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
