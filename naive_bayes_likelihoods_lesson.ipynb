{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b01b39fd-4b20-468d-88a4-8d60080fd770",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson L1 (Week 2) â€“ Visualising Naive Bayes Likelihoods ğŸ“ˆğŸ”µğŸ”´\n",
    "\n",
    "Welcome to **Week 2, Lesson L1** of the *Practical NLP for Beginners* miniâ€‘course!\n",
    "\n",
    "### ğŸ‘©ğŸ»â€ğŸ« What youâ€™ll learn in the next 30 minutes\n",
    "* **Recap the Naive Bayes classifier** and why logâ€‘likelihoods are handy.\n",
    "* **Generate features onâ€‘theâ€‘fly** â€” no external CSV needed.\n",
    "* **Plot tweets in likelihood space** and see the decision boundary appear.\n",
    "* **Interpret confidence ellipses** around positive vs. negative sentiment.\n",
    "* **Play with an interactive explorer** and watch probabilities update live.\n",
    "\n",
    "> **Prerequisites:** only basic Python and NumPy.  \n",
    "> **Goal:** gain an *intuitive* feel for how Naive Bayes classifies text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b173a4b-70b9-46ad-a5ca-4330cc3f21bd",
   "metadata": {},
   "source": [
    "\n",
    "---  \n",
    "### ğŸ†• Whatâ€™s different from the original Coursera lab?\n",
    "| Change | Why it matters |\n",
    "|--------|----------------|\n",
    "| **Features generated inâ€‘notebook** | Everything is reproducible â€” no `bayes_features.csv` download. |\n",
    "| **Courseraâ€‘compatible names** | You can still follow Andrewâ€™s videos lineâ€‘byâ€‘line. |\n",
    "| **Tiny toy walkâ€‘through** | The maths becomes tangible before we scale up. |\n",
    "| **Interactive Gradio explorer** | Probe single tweets and *see* the numbers change. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6671115-7ec1-4789-9520-ec47ba1ebd3d",
   "metadata": {},
   "source": [
    "## ğŸ€Â 0 â€“ Environment setâ€‘up (â‰ˆÂ 1â€¯min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de63ad5-2678-42df-bf54-f014d162c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ€Â SetupÂ â€“ installs take ~45â€¯s â³\n",
    "!pip -q install --upgrade nltk wordcloud gradio>=4.27.0 numpy>=1.26,<2.1 scikit-learn<1.7 websockets>=13,<15 --progress-bar off\n",
    "\n",
    "import nltk, ssl, warnings; warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "for res in ['stopwords', 'punkt', 'twitter_samples']:\n",
    "    nltk.download(res, quiet=True)\n",
    "print('âœ…Â Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fe063-34ae-45f2-8383-38b6cb67fa42",
   "metadata": {},
   "source": [
    "\n",
    "That single cell:\n",
    "\n",
    "1. Installs the **only external libraries** we need.  \n",
    "2. Downloads three corpora from *NLTK*.  \n",
    "3. Prints a green check âœ”ï¸ when done.\n",
    "\n",
    "> **Tip:** if the download fails once, just rerun â€” it is cached after the first success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7617267-349d-4924-b021-bf1083bb4102",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£Â Toy example â€“ six tiny tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21cbdb-814d-4a66-9df2-13561dff2c82",
   "metadata": {},
   "source": [
    "\n",
    "We warm up with *six* twoâ€‘word tweets:\n",
    "\n",
    "```text\n",
    "Positive â†’  love it | so happy | great day\n",
    "Negative â†’  hate it | so sad   | bad day\n",
    "```\n",
    "\n",
    "A microscopic dataset means we can **calculate every probability by hand** and *see* how Laplace smoothing nudges them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b4d2c-c0c7-4a9c-b62d-3d3357b171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re, matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def clean(t: str):\n",
    "    \"\"\"Lowerâ€‘case â†’ remove nonâ€‘letters â†’ tokenize â†’ stopâ€‘word filter â†’ stem\"\"\"\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'[^a-z\\s]', '', t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "# Tiny corpus\n",
    "t_pos = [\"love it\", \"so happy\", \"great day\"]\n",
    "t_neg = [\"hate it\", \"so sad\", \"bad day\"]\n",
    "tweets = t_pos + t_neg\n",
    "ys = np.array([1]*3 + [0]*3)  # 1Â =Â pos,Â 0Â =Â neg\n",
    "\n",
    "# Likelihood helpers --------------------------------------------------------\n",
    "from collections import Counter\n",
    "def word_likelihoods(pos, neg, alpha=1):\n",
    "    pos_counts = Counter(w for tw in pos for w in clean(tw))\n",
    "    neg_counts = Counter(w for tw in neg for w in clean(tw))\n",
    "    vocab = set(pos_counts)|set(neg_counts); V = len(vocab)\n",
    "    tot_pos = sum(pos_counts.values()) + alpha*V\n",
    "    tot_neg = sum(neg_counts.values()) + alpha*V\n",
    "    lp = {w: np.log((pos_counts[w]+alpha)/tot_pos) for w in vocab}\n",
    "    ln = {w: np.log((neg_counts[w]+alpha)/tot_neg) for w in vocab}\n",
    "    return lp, ln\n",
    "\n",
    "lp, ln = word_likelihoods(t_pos, t_neg)\n",
    "\n",
    "def tweet_ll(t):\n",
    "    toks = clean(t)\n",
    "    lpos = sum(lp.get(w, 0) for w in toks)\n",
    "    lneg = sum(ln.get(w, 0) for w in toks)\n",
    "    return lpos, lneg\n",
    "\n",
    "XY = np.array([tweet_ll(t) for t in tweets])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(XY[:3,0], XY[:3,1], c='green', label='pos')\n",
    "plt.scatter(XY[3:,0], XY[3:,1], c='red', label='neg')\n",
    "plt.axline((0,0), (1,1), ls='--', c='grey')\n",
    "plt.xlabel('log P(t|pos)'); plt.ylabel('log P(t|neg)')\n",
    "plt.title('Tinyâ€‘tweet likelihood space'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d716c9-bfff-406f-9656-9a61531db22e",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ¨ **Reading the picture**\n",
    "\n",
    "* Each dot is a **tweet** embedded in 2â€‘D: \\((\\log P(t\\mid\\text{pos}),\\log P(t\\mid\\text{neg}))\\).\n",
    "* The **grey dashed line** is where the two logs are equal â€” our *decision boundary* when priors are 50 / 50.\n",
    "* Points below the line have larger positive likelihood â†’ classified *positive*, and viceâ€‘versa.\n",
    "\n",
    "> **Play:** change `alpha` in `word_likelihoods` or modify a tweet, rerun, and watch the dots move!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c229059-bdeb-4e45-ac92-60f2a6555b4c",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£Â Helper functions for the full corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740e36e-89b1-4c11-a0e1-1fac2820ded8",
   "metadata": {},
   "source": [
    "\n",
    "The next cell builds two helpers:\n",
    "\n",
    "1. **`process_tweet`** â€“ lowerâ€‘cases, strips URLs, removes punctuation & stopâ€‘words, then stems.  \n",
    "2. **`build_likelihoods`** â€“ computes smoothed logâ€‘likelihood dictionaries for *all* words.\n",
    "\n",
    "Feel free to tweak the preâ€‘processing or smoothing factor *Î±*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7006abd-d60a-4705-97f1-670e07f5dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from collections import Counter\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "\n",
    "def process_tweet(t: str):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'https?://\\S+', '', t)\n",
    "    t = re.sub(r'[^a-z\\s]', '', t)\n",
    "    return [stemmer.stem(w) for w in t.split() if w not in stop_words]\n",
    "\n",
    "def build_likelihoods(tweets, ys, alpha=1):\n",
    "    pos = [tw for tw, y in zip(tweets, ys) if y == 1]\n",
    "    neg = [tw for tw, y in zip(tweets, ys) if y == 0]\n",
    "    pos_c = Counter(w for tw in pos for w in process_tweet(tw))\n",
    "    neg_c = Counter(w for tw in neg for w in process_tweet(tw))\n",
    "    vocab = set(pos_c)|set(neg_c); V = len(vocab)\n",
    "    tot_p = sum(pos_c.values()) + alpha*V\n",
    "    tot_n = sum(neg_c.values()) + alpha*V\n",
    "    lp = {w: np.log((pos_c[w]+alpha)/tot_p) for w in vocab}\n",
    "    ln = {w: np.log((neg_c[w]+alpha)/tot_n) for w in vocab}\n",
    "    return lp, ln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c11bd-e71c-4658-a3a0-1a1f0d717728",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£Â Realâ€‘corpus likelihood scatter & confidence ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792d563-3728-491c-a0ee-41092f20e3db",
   "metadata": {},
   "source": [
    "\n",
    "The NLTK Twitter corpus has 5 000 positive and 5 000 negative tweets labelled by crowdâ€‘workers.\n",
    "\n",
    "Weâ€™ll plot a **random sample of 2 000** and draw **95 % confidence ellipses** so the class shapes are still visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725df04-90cd-4f41-a5eb-d491fed28f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = twitter_samples.strings('positive_tweets.json')\n",
    "tweets_neg = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = tweets_pos + tweets_neg\n",
    "ys = np.array([1]*len(tweets_pos) + [0]*len(tweets_neg))\n",
    "\n",
    "lp, ln = build_likelihoods(tweets, ys)\n",
    "\n",
    "def tweet_ll_real(t):\n",
    "    toks = process_tweet(t)\n",
    "    return sum(lp.get(w,0) for w in toks), sum(ln.get(w,0) for w in toks)\n",
    "\n",
    "XY_full = np.array([tweet_ll_real(t) for t in tweets])\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(len(tweets), 2000, replace=False)\n",
    "XY = XY_full[idx]; y_s = ys[idx]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(XY[y_s==1,0], XY[y_s==1,1], s=10, c='limegreen', label='pos', alpha=0.5)\n",
    "ax.scatter(XY[y_s==0,0], XY[y_s==0,1], s=10, c='crimson', label='neg', alpha=0.5)\n",
    "ax.axline((0,0), (1,1), ls='--', c='grey')\n",
    "\n",
    "def ellipse(data, color):\n",
    "    import numpy.linalg as LA\n",
    "    cov = np.cov(data, rowvar=False); mean = data.mean(axis=0)\n",
    "    vals, vecs = LA.eigh(cov); order = vals.argsort()[::-1]\n",
    "    vals, vecs = vals[order], vecs[:,order]\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    width, height = 2*np.sqrt(vals*5.991)  # Ï‡2 0.95, df=2\n",
    "    ell = Ellipse(mean, width, height, theta, edgecolor=color,\n",
    "                  facecolor='none', lw=2)\n",
    "    ax.add_patch(ell)\n",
    "\n",
    "ellipse(XY[y_s==1], 'green'); ellipse(XY[y_s==0], 'red')\n",
    "ax.set_xlabel('log P(t|pos)'); ax.set_ylabel('log P(t|neg)')\n",
    "ax.set_title('Likelihood space â€“ 2â€¯000â€‘tweet sample'); ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e228eb-070e-4f90-9f6f-5b48f6369863",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ” **Interpreting the ellipses**\n",
    "\n",
    "* They enclose approximately 95 % of tweets *if* the class distributions were Gaussian.  \n",
    "* **Tilt** â†’ correlation between the two logâ€‘likelihoods.  \n",
    "* **Area** â†’ overall variance (here, negative tweets are slightly more spread).\n",
    "\n",
    "> **Exercise:** bump the sample size from 2 000 to 10 000 (all tweets). Does the plot look more or less Gaussian?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3865eaa-be03-429f-962f-bd9f5e649ad3",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£Â Interactive likelihood explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3350386-b455-4256-8e71-8361b614ab5d",
   "metadata": {},
   "source": [
    "\n",
    "Type *any* sentence below and watch the numbers appear.\n",
    "\n",
    "| Field | Meaning |\n",
    "|-------|---------|\n",
    "| **log P(t\\|pos)** | Sum of perâ€‘word logs under the positive model |\n",
    "| **log P(t\\|neg)** | Same, but negative |\n",
    "| **Posterior P(pos)** | Assuming equal class priors |\n",
    "| **Prediction** | Emojiâ€‘level TL;DR |\n",
    "\n",
    "Try:\n",
    "\n",
    "* â€œI absolutely adore this!â€ â†’ should go green.  \n",
    "* â€œThis is the worst thing ever.â€ â†’ likely red.  \n",
    "* â€œNothing special, meh.â€ â†’ often near 0.5, showing model uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700e6fb-f41a-4f23-8f01-2367c9cc1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def explorer(txt):\n",
    "    lpv, lnv = tweet_ll_real(txt)\n",
    "    prob = 1 / (1 + np.exp(lnv - lpv))\n",
    "    return {\n",
    "        \"log P(t|pos)\": round(lpv, 3),\n",
    "        \"log P(t|neg)\": round(lnv, 3),\n",
    "        \"Posterior P(pos)\": round(prob, 3),\n",
    "        \"Prediction\": \"Positive ğŸ˜Š\" if prob >= 0.5 else \"Negative ğŸ˜\"\n",
    "    }\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('### ğŸ•µï¸Â Likelihood explorer (NaiveÂ Bayes)')\n",
    "    inp = gr.Textbox(lines=3, label='Tweet text')\n",
    "    out = gr.JSON()\n",
    "    inp.submit(explorer, inp, out)\n",
    "    gr.Button('Run').click(explorer, inp, out)\n",
    "\n",
    "# Uncomment when running locally or on Colab:\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c8dd0-76d2-461a-b876-cbd4043e186e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **Congratulations!** You visualised Naive Bayes likelihoods, interpreted class uncertainty, and built an interactive demoâ€”*all in one notebook*.\n",
    "\n",
    "### âœï¸ Miniâ€‘project\n",
    "1. Replace the Twitter corpus with **your own data** (e.g. IMDB reviews).  \n",
    "2. Reâ€‘train `build_likelihoods`.  \n",
    "3. Plot the new space â€” does the separation get cleaner or messier?  \n",
    "4. Share a screenshot with the community!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
