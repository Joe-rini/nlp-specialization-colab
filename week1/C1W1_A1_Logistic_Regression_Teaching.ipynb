{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f34ccac8",
      "metadata": {
        "id": "f34ccac8"
      },
      "source": [
        "# Assignment A1 – Building Logistic Regression from Scratch 🔧📈  \n",
        "\n",
        "This teaching notebook **wraps the original Coursera Assignment A1** with extra scaffolding.\n",
        "\n",
        "> **Goal**: implement gradient descent for logistic regression and apply it to tweet‑sentiment classification.  \n",
        "\n",
        "This assignment takes the concepts learned in the lessons, and builds the code and some of the math intuition. Depending on your learning goals, you may not need to learn how to code this assignment.\n",
        "\n",
        "✅ What’s new in the assignment versus the lessons:\n",
        "- The assignment asks you to implement it yourself step by step:\n",
        "- Implement sigmoid()\n",
        "- Manually compute predictions and gradients\n",
        "- Write your own gradientDescent() function\n",
        "- Make predictions from scratch\n",
        "- Evaluate accuracy without using sklearn\n",
        "\n",
        "🧠 Teaching goal:\n",
        "- While the lessons explain and visualize the concepts, the assignment ensures you:\n",
        "- Understand the math of logistic regression\n",
        "- Practice vectorizing operations\n",
        "- Learn how models are trained through iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24a58de",
      "metadata": {
        "id": "b24a58de"
      },
      "outputs": [],
      "source": [
        "# 🍀 Colab setup – run once\n",
        "!pip -q install --upgrade \\\n",
        "        \"numpy>=1.26,<2.1\" \\\n",
        "        \"scikit-learn<1.7\" \\\n",
        "        \"nltk\" \\\n",
        "        \"wordcloud\" \\\n",
        "        \"gradio>=4.27.0\" \\\n",
        "        \"websockets>=13,<15\" --progress-bar off\n",
        "\n",
        "import nltk, ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "for res in ['stopwords','punkt','twitter_samples']:\n",
        "    nltk.download(res, quiet=True)\n",
        "\n",
        "print('✅ Environment ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33523ebd",
      "metadata": {
        "id": "33523ebd"
      },
      "source": [
        "## 1️⃣ Toy walkthrough (6 tweets)  \n",
        "\n",
        "Before diving into full gradient descent, let’s solve a *mini* version with scikit‑learn so you can **see the expected behaviour**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b22dbaa",
      "metadata": {
        "id": "8b22dbaa"
      },
      "outputs": [],
      "source": [
        "import numpy as np, re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
        "def clean(text):\n",
        "    text = text.lower(); text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return [stemmer.stem(w) for w in text.split() if w not in stop_words]\n",
        "\n",
        "pos_lex, neg_lex = {'love','great','happy'}, {'hate','bad','sad'}\n",
        "def to_xy(tokens):\n",
        "    return [sum(w in pos_lex for w in tokens), sum(w in neg_lex for w in tokens)]\n",
        "\n",
        "tiny = [\"I love this\", \"So happy!\", \"Great product\",\n",
        "        \"I hate this\", \"Very bad\", \"So sad\"]\n",
        "y_tiny = np.array([1,1,1,0,0,0])\n",
        "X_tiny = np.array([to_xy(clean(t)) for t in tiny])\n",
        "\n",
        "clf = LogisticRegression(); clf.fit(X_tiny, y_tiny)\n",
        "print(\"Tiny accuracy:\", clf.score(X_tiny, y_tiny))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(4,4))\n",
        "for lab, m, c in [(1,'o','green'), (0,'x','red')]:\n",
        "    sel = y_tiny==lab\n",
        "    plt.scatter(X_tiny[sel,0], X_tiny[sel,1], marker=m, color=c)\n",
        "coef, b = clf.coef_[0], clf.intercept_[0]\n",
        "xs = np.linspace(-0.2,3,100); plt.plot(xs, -(coef[0]*xs + b)/coef[1], '--k')\n",
        "plt.xlabel('Positive'); plt.ylabel('Negative'); plt.title('Toy LR boundary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f7323c",
      "metadata": {
        "id": "77f7323c"
      },
      "source": [
        "**Take‑away:** the straight boundary is what you’ll reproduce with *your* implementation in Section 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0067302",
      "metadata": {
        "id": "b0067302"
      },
      "source": [
        "## 2️⃣ Helper code – cleaning & feature building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a56332",
      "metadata": {
        "id": "c5a56332"
      },
      "outputs": [],
      "source": [
        "import numpy as np, re, math\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stop_words, stemmer = set(stopwords.words('english')), PorterStemmer()\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r'https?://\\S+', '', tweet)\n",
        "    tweet = re.sub(r'[^a-z\\s]', '', tweet)\n",
        "    return [stemmer.stem(w) for w in tweet.split() if w not in stop_words]\n",
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    freqs = {}\n",
        "    for y, t in zip(ys, tweets):\n",
        "        for w in process_tweet(t):\n",
        "            freqs[(w, y)] = freqs.get((w, y), 0) + 1\n",
        "    return freqs\n",
        "\n",
        "def extract_features(tweet, freqs):\n",
        "    '''Return [1, pos_count, neg_count] for one tweet'''\n",
        "    x = np.zeros(3)\n",
        "    x[0] = 1\n",
        "    for w in process_tweet(tweet):\n",
        "        x[1] += freqs.get((w,1.0),0)\n",
        "        x[2] += freqs.get((w,0.0),0)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f06c28",
      "metadata": {
        "id": "87f06c28"
      },
      "source": [
        "## 3️⃣ Prepare full tweet dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b29785c",
      "metadata": {
        "id": "0b29785c"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "tweets = pos_tweets + neg_tweets\n",
        "ys = np.append(np.ones(len(pos_tweets)), np.zeros(len(neg_tweets)))\n",
        "\n",
        "freqs = build_freqs(tweets, ys)\n",
        "print(\"Frequency dict size:\", len(freqs))\n",
        "\n",
        "# Build feature matrix\n",
        "X = np.stack([extract_features(t, freqs) for t in tweets])\n",
        "print(\"Feature matrix shape:\", X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f804f038",
      "metadata": {
        "id": "f804f038"
      },
      "source": [
        "## 4️⃣ Your turn – implement Logistic Regression with Gradient Descent  \n",
        "\n",
        "We’ll guide you through:  \n",
        "\n",
        "1. **Sigmoid** function  \n",
        "2. Cost computation  \n",
        "3. Gradient calculation  \n",
        "4. Parameter update in loop  \n",
        "\n",
        "👉 **Fill the TODOs** below (solutions hidden)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0676b313",
      "metadata": {
        "id": "0676b313"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"Compute sigmoid – ***complete this***\"\"\"\n",
        "    ### TODO\n",
        "    return 1/(1+np.exp(-z))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e95dbc7",
      "metadata": {
        "id": "0e95dbc7"
      },
      "outputs": [],
      "source": [
        "def compute_cost_and_grad(theta, X, y):\n",
        "    m = len(y)\n",
        "    z = np.dot(X, theta)\n",
        "    h = sigmoid(z)\n",
        "    cost = -(1/m)*(np.dot(y, np.log(h)) + np.dot(1-y, np.log(1-h)))\n",
        "    grad = (1/m)*np.dot(X.T, (h - y))\n",
        "    return cost, grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d7ab97",
      "metadata": {
        "id": "44d7ab97"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, alpha=1e-9, iters=1500):\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    costs = []\n",
        "    for i in range(iters):\n",
        "        cost, grad = compute_cost_and_grad(theta, X, y)\n",
        "        theta -= alpha * grad\n",
        "        if i%100==0: costs.append(cost)\n",
        "    return theta, costs\n",
        "\n",
        "theta, costs = gradient_descent(X, ys, alpha=1e-9, iters=2000)\n",
        "print(\"Final cost:\", costs[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f37a1b3",
      "metadata": {
        "id": "5f37a1b3"
      },
      "source": [
        "### Cost curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d984d0e2",
      "metadata": {
        "id": "d984d0e2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(costs); plt.title('Cost over iterations'); plt.xlabel('Every 100 steps'); plt.ylabel('Cost'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7884e98",
      "metadata": {
        "id": "b7884e98"
      },
      "source": [
        "## 5️⃣ Evaluate your handmade model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5508fc68",
      "metadata": {
        "id": "5508fc68"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = sigmoid(np.dot(X, theta)) >= 0.5\n",
        "print(\"Hand‑built LR accuracy:\", accuracy_score(ys, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff142b9",
      "metadata": {
        "id": "bff142b9"
      },
      "source": [
        "## 6️⃣ Gradio tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f214762",
      "metadata": {
        "id": "1f214762"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict(text):\n",
        "    x = extract_features(text, freqs)\n",
        "    prob = float(sigmoid(np.dot(x, theta)))\n",
        "    label = \"Positive 😊\" if prob>=0.5 else \"Negative 😞\"\n",
        "    return {\"Prob‑positive\": round(prob,3), \"Prediction\": label}\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Test your gradient‑descent model\")\n",
        "    inp = gr.Textbox(lines=3, label=\"Tweet text\")\n",
        "    out = gr.JSON()\n",
        "    inp.submit(predict, inp, out)\n",
        "    gr.Button(\"Run\").click(predict, inp, out)\n",
        "\n",
        "# Uncomment when running in Colab\n",
        "# demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d26522",
      "metadata": {
        "id": "f2d26522"
      },
      "source": [
        "---\n",
        "\n",
        "🎯 **You built Logistic Regression from scratch and deployed it!**  \n",
        "\n",
        "Next: experiment with learning rates, iteration counts, or add L2 regularisation to improve stability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}