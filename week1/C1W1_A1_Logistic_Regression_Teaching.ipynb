{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34ccac8",
   "metadata": {},
   "source": [
    "# Assignmentâ€¯A1Â â€“Â Building Logistic Regression from ScratchÂ ðŸ”§ðŸ“ˆ  \n",
    "\n",
    "This teaching notebook **wraps the original Coursera Assignment A1** with extra scaffolding:\n",
    "\n",
    "* Clear learning goals & roadmap  \n",
    "* Colabâ€‘ready `pip` installs (no external files)  \n",
    "* **Toy walkthrough** that solves a microâ€‘version of the task so you see the whole pipeline first  \n",
    "* Rich markdown explanations for each cell  \n",
    "* A small **Gradio tester** at the end so you can play with your model  \n",
    "\n",
    "> **Goal**: implement gradient descent for logistic regression and apply it to tweetâ€‘sentiment classification.  \n",
    "\n",
    "_Ready?Â Letâ€™s build some classifiers!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ€Â Colab setup â€“ run once\n",
    "!pip -q install --upgrade \\\n",
    "        \"numpy>=1.26,<2.1\" \\\n",
    "        \"scikit-learn<1.7\" \\\n",
    "        \"nltk\" \\\n",
    "        \"wordcloud\" \\\n",
    "        \"gradio>=4.27.0\" \\\n",
    "        \"websockets>=13,<15\" --progress-bar off\n",
    "\n",
    "import nltk, ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "for res in ['stopwords','punkt','twitter_samples']:\n",
    "    nltk.download(res, quiet=True)\n",
    "\n",
    "print('âœ…Â Environment ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33523ebd",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£Â Toy walkthrough (6 tweets)  \n",
    "\n",
    "Before diving into full gradient descent, letâ€™s solve a *mini* version with scikitâ€‘learn so you can **see the expected behaviour**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stemmer, stop_words = PorterStemmer(), set(stopwords.words('english'))\n",
    "def clean(text):\n",
    "    text = text.lower(); text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return [stemmer.stem(w) for w in text.split() if w not in stop_words]\n",
    "\n",
    "pos_lex, neg_lex = {'love','great','happy'}, {'hate','bad','sad'}\n",
    "def to_xy(tokens):\n",
    "    return [sum(w in pos_lex for w in tokens), sum(w in neg_lex for w in tokens)]\n",
    "\n",
    "tiny = [\"I love this\", \"So happy!\", \"Great product\",\n",
    "        \"I hate this\", \"Very bad\", \"So sad\"]\n",
    "y_tiny = np.array([1,1,1,0,0,0])\n",
    "X_tiny = np.array([to_xy(clean(t)) for t in tiny])\n",
    "\n",
    "clf = LogisticRegression(); clf.fit(X_tiny, y_tiny)\n",
    "print(\"Tiny accuracy:\", clf.score(X_tiny, y_tiny))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(4,4))\n",
    "for lab, m, c in [(1,'o','green'), (0,'x','red')]:\n",
    "    sel = y_tiny==lab\n",
    "    plt.scatter(X_tiny[sel,0], X_tiny[sel,1], marker=m, color=c)\n",
    "coef, b = clf.coef_[0], clf.intercept_[0]\n",
    "xs = np.linspace(-0.2,3,100); plt.plot(xs, -(coef[0]*xs + b)/coef[1], '--k')\n",
    "plt.xlabel('Positive'); plt.ylabel('Negative'); plt.title('Toy LR boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7323c",
   "metadata": {},
   "source": [
    "**Takeâ€‘away:** the straight boundary is what youâ€™ll reproduce with *your* implementation in SectionÂ 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0067302",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£Â Helper code â€“ cleaning & feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a56332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re, math\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words, stemmer = set(stopwords.words('english')), PorterStemmer()\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?://\\S+', '', tweet)\n",
    "    tweet = re.sub(r'[^a-z\\s]', '', tweet)\n",
    "    return [stemmer.stem(w) for w in tweet.split() if w not in stop_words]\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    freqs = {}\n",
    "    for y, t in zip(ys, tweets):\n",
    "        for w in process_tweet(t):\n",
    "            freqs[(w, y)] = freqs.get((w, y), 0) + 1\n",
    "    return freqs\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    '''Return [1, pos_count, neg_count] for one tweet'''\n",
    "    x = np.zeros(3)\n",
    "    x[0] = 1\n",
    "    for w in process_tweet(tweet):\n",
    "        x[1] += freqs.get((w,1.0),0)\n",
    "        x[2] += freqs.get((w,0.0),0)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f06c28",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£Â Prepare full tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = pos_tweets + neg_tweets\n",
    "ys = np.append(np.ones(len(pos_tweets)), np.zeros(len(neg_tweets)))\n",
    "\n",
    "freqs = build_freqs(tweets, ys)\n",
    "print(\"Frequency dict size:\", len(freqs))\n",
    "\n",
    "# Build feature matrix\n",
    "X = np.stack([extract_features(t, freqs) for t in tweets])\n",
    "print(\"Feature matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804f038",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£Â Your turn â€“ implement Logistic Regression with Gradient Descent  \n",
    "\n",
    "Weâ€™ll guide you through:  \n",
    "\n",
    "1. **Sigmoid** function  \n",
    "2. Cost computation  \n",
    "3. Gradient calculation  \n",
    "4. Parameter update in loop  \n",
    "\n",
    "ðŸ‘‰ **Fill the TODOs** below (solutions hidden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute sigmoid â€“ ***complete this***\"\"\"\n",
    "    ###Â TODO \n",
    "    return 1/(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e95dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_grad(theta, X, y):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    cost = -(1/m)*(np.dot(y, np.log(h)) + np.dot(1-y, np.log(1-h)))\n",
    "    grad = (1/m)*np.dot(X.T, (h - y))\n",
    "    return cost, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=1e-9, iters=1500):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    costs = []\n",
    "    for i in range(iters):\n",
    "        cost, grad = compute_cost_and_grad(theta, X, y)\n",
    "        theta -= alpha * grad\n",
    "        if i%100==0: costs.append(cost)\n",
    "    return theta, costs\n",
    "\n",
    "theta, costs = gradient_descent(X, ys, alpha=1e-9, iters=2000)\n",
    "print(\"Final cost:\", costs[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37a1b3",
   "metadata": {},
   "source": [
    "### Cost curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(costs); plt.title('Cost over iterations'); plt.xlabel('Every 100 steps'); plt.ylabel('Cost'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7884e98",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£Â Evaluate your handmade model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = sigmoid(np.dot(X, theta)) >= 0.5\n",
    "print(\"Handâ€‘built LR accuracy:\", accuracy_score(ys, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff142b9",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£Â Gradio tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f214762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(text):\n",
    "    x = extract_features(text, freqs)\n",
    "    prob = float(sigmoid(np.dot(x, theta)))\n",
    "    label = \"Positive ðŸ˜Š\" if prob>=0.5 else \"Negative ðŸ˜ž\"\n",
    "    return {\"Probâ€‘positive\": round(prob,3), \"Prediction\": label}\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### Test your gradientâ€‘descent model\")\n",
    "    inp = gr.Textbox(lines=3, label=\"Tweet text\")\n",
    "    out = gr.JSON()\n",
    "    inp.submit(predict, inp, out)\n",
    "    gr.Button(\"Run\").click(predict, inp, out)\n",
    "\n",
    "# Uncomment when running in Colab\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d26522",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ðŸŽ¯ **You built Logistic Regression from scratch and deployed it!**  \n",
    "\n",
    "Next: experiment with learning rates, iteration counts, or add L2 regularisation to improve stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
