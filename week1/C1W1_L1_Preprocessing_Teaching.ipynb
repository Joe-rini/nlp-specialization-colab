{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "faffc02c",
      "metadata": {
        "id": "faffc02c"
      },
      "source": [
        "# Lesson L1: Natural Language Preâ€‘processing ðŸŒŸ\n",
        "\n",
        "Welcome to **Lesson 1** of the *DeepLearning.AI NLP Specialization* teaching notebook.  \n",
        "This notebook shows you **how to clean and prepare raw text** so it can be fed to classical machineâ€‘learning algorithms *and* modern neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## Why does text preprocessing matter? ðŸ¤”\n",
        "* Raw text is messy â€“ full of punctuation, contractions, emojis, hyperlinks and *noise* that distract a model.  \n",
        "* Careful preprocessing lets the model focus on **signal**, improving accuracy, reducing vocabulary size and training time.\n",
        "\n",
        "---\n",
        "\n",
        "## What you'll do in this notebook  ðŸ—ºï¸\n",
        "\n",
        "1. **Build intuition with a toy example** â€“ walk a single sentence through tokenization âž¡ï¸ stopâ€‘word filtering âž¡ï¸ stemming.  \n",
        "2. **Peek at a real dataset** â€“ load the NLTK Twitter corpus and inspect its structure.  \n",
        "3. **Implement a complete preprocessing pipeline** you can reuse in later lessons and assignments.  \n",
        "4. **Play in an interactive Gradio demo** â€“ paste any tweet and watch each preprocessing step in realâ€‘time.\n",
        "\n",
        "> ðŸ‘‰ Feel free to run each cell, tweak the code and break things. That's the fastest way to learn!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8abe2c",
      "metadata": {
        "id": "3a8abe2c"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall \"numpy<2.0\" \"scikit-learn<1.7\" \"nltk==3.8.1\" \"gradio==4.27.0\" --progress-bar off\n",
        "\n",
        "# @title Install & import required libraries\n",
        "# Colab automatically skips installations that already exist\n",
        "!pip install --upgrade \"numpy<2.0\" \"scikit-learn<1.7\" \"nltk==3.8.1\" \"gradio==4.27.0\" --progress-bar off\n",
        "\n",
        "import nltk, re, string, random\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download small NLTK resources ðŸ“¥\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffa4a3e",
      "metadata": {
        "id": "4ffa4a3e"
      },
      "source": [
        "## 1ï¸âƒ£ Toy example â€“ preprocessing one sentence\n",
        "\n",
        "Before diving into thousands of tweets, let's warmâ€‘up with **one short sentence**.  \n",
        "We'll walk through each preprocessing step so you can *see* what changes and *why* it matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f080a1b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f080a1b7",
        "outputId": "34bf49f8-f41b-45d3-827a-d78af059907b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¸ Original sentence:\n",
            "I love Natural Language Processing! :) #NLP\n",
            "\n",
            "ðŸ”¹ After tokenization:\n",
            "['i', 'love', 'natural', 'language', 'processing', '!', ':)', '#nlp']\n",
            "\n",
            "ðŸ”¹ After stopâ€‘word & punctuation filtering:\n",
            "['love', 'natural', 'language', 'processing', ':)', '#nlp']\n",
            "\n",
            "ðŸ”¹ After stemming:\n",
            "['love', 'natur', 'languag', 'process', ':)', '#nlp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk, string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Make sure NLTK data is available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Your example sentence\n",
        "sentence = \"I love Natural Language Processing! :) #NLP\"\n",
        "\n",
        "print(\"ðŸ”¸ Original sentence:\")\n",
        "print(sentence)\n",
        "\n",
        "# 1. Tokenize\n",
        "tokenizer = TweetTokenizer(preserve_case=False,\n",
        "                           strip_handles=True,\n",
        "                           reduce_len=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"\\nðŸ”¹ After tokenization:\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Remove stopâ€‘words & punctuation\n",
        "stopwords_en = stopwords.words('english')\n",
        "tokens_no_sw = [w for w in tokens if w not in stopwords_en and w not in string.punctuation]\n",
        "print(\"\\nðŸ”¹ After stopâ€‘word & punctuation filtering:\")\n",
        "print(tokens_no_sw)\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in tokens_no_sw]\n",
        "print(\"\\nðŸ”¹ After stemming:\")\n",
        "print(stemmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16d35938",
      "metadata": {
        "id": "16d35938"
      },
      "source": [
        "**What happened?**\n",
        "\n",
        "* `TweetTokenizer` **standardises** the text â€“ lowerâ€‘casing, normalising elongated words *soooo â†’ so*, and removing Twitter handles.  \n",
        "* Stopâ€‘words like *'i', 'love'* (which carry little meaning for sentiment analysis) are filtered out.  \n",
        "* Finally, **stemming** reduces words to a common root (*processing â†’ process*), shrinking the vocabulary your model has to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8a24bc",
      "metadata": {
        "id": "ba8a24bc"
      },
      "source": [
        "## 2ï¸âƒ£ Exploring the Twitter sentiment dataset\n",
        "\n",
        "We'll use the **`twitter_samples`** corpus that ships with NLTK.  \n",
        "It contains **5,000 positive** and **5,000 negative** English tweets originally collected by researchers at the University of Michigan.\n",
        "\n",
        "Let's load the text and take a quick look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14ad3ab",
      "metadata": {
        "id": "a14ad3ab"
      },
      "outputs": [],
      "source": [
        "# Load positive & negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(f\"âœ… Loaded {len(all_positive_tweets):,} positive tweets\")\n",
        "print(f\"âœ… Loaded {len(all_negative_tweets):,} negative tweets\")\n",
        "\n",
        "# Quick sanity check â€“ view one random tweet from each class\n",
        "print(\"\\nðŸ”¸ Example positive tweet:\")\n",
        "print(random.choice(all_positive_tweets))\n",
        "print(\"\\nðŸ”¸ Example negative tweet:\")\n",
        "print(random.choice(all_negative_tweets))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff307e13",
      "metadata": {
        "id": "ff307e13"
      },
      "source": [
        "*Notice the hashtags, emojis, URLs and random user mentions.*  \n",
        "We'll need to clean all that up before feeding the text to a model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5142e16",
      "metadata": {
        "id": "d5142e16"
      },
      "source": [
        "## 3ï¸âƒ£ Building a reusable `process_tweet()` function\n",
        "\n",
        "Instead of repeating the three steps above every time, let's wrap them in a **single helper** you can call from future notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b482285e",
      "metadata": {
        "id": "b482285e"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet: str):\n",
        "    \"\"\"Preprocess a single tweet into a list of cleaned, stemmed tokens.\n",
        "\n",
        "    Steps\n",
        "    -----\n",
        "    1. Lowerâ€‘case and remove hyperlinks, hashtags & handles.\n",
        "    2. Tokenize with `TweetTokenizer`.\n",
        "    3. Filter stopâ€‘words & punctuation.\n",
        "    4. Stem remaining tokens.\n",
        "    \"\"\"\n",
        "    # 1. Normalise text\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)   # remove links\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)                     # remove handles\n",
        "    tweet = re.sub(r'#', '', tweet)                         # strip hashtag symbol\n",
        "\n",
        "    # 2. Tokenise\n",
        "    tokenizer = TweetTokenizer(preserve_case=False,\n",
        "                               strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # 3. Remove stopwords & punctuation\n",
        "    stopwords_en = stopwords.words('english')\n",
        "    tokens_clean = [tok for tok in tokens\n",
        "                    if tok not in stopwords_en\n",
        "                    and tok not in string.punctuation]\n",
        "\n",
        "    # 4. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(tok) for tok in tokens_clean]\n",
        "\n",
        "    return stems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c9abfd",
      "metadata": {
        "id": "a2c9abfd"
      },
      "outputs": [],
      "source": [
        "sample = all_positive_tweets[2277]\n",
        "print(\"Original tweet:\\n\", sample)\n",
        "\n",
        "print(\"\\nProcessed tokens:\")\n",
        "print(process_tweet(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb63df5",
      "metadata": {
        "id": "fbb63df5"
      },
      "source": [
        "âœ¨ *Much cleaner!* You can now vectorise these tokens (e.g., with TFâ€‘IDF or word embeddings) and train a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c89956",
      "metadata": {
        "id": "51c89956"
      },
      "source": [
        "## 4ï¸âƒ£ Interactive playground ðŸ”§\n",
        "\n",
        "Use the widget below to **experiment** â€“ type any short snippet and see exactly what `process_tweet()` returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1926c31d",
      "metadata": {
        "id": "1926c31d"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Helper for the demo\"\"\"\n",
        "    return ' '.join(process_tweet(text))\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=preprocess,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type a tweet here...\"),\n",
        "    outputs=gr.Textbox(label=\"Processed tokens\"),\n",
        "    title=\"Tweet Preâ€‘processor\",\n",
        "    description=\"Watch how raw text is cleaned, tokenised and stemmed.\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=False, share=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6085cc8a",
      "metadata": {
        "id": "6085cc8a"
      },
      "source": [
        "---\n",
        "\n",
        "### ðŸŽ‰ You finished Lessonâ€¯L1!\n",
        "\n",
        "You're now equipped to **clean tweets** for sentiment analysis and beyond.  \n",
        "In the next lesson you'll build features from these tokens and train your first classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}