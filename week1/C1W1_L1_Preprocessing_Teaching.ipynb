{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faffc02c",
   "metadata": {},
   "source": [
    "# Lesson L1: Natural Language Pre‚Äëprocessing üåü\n",
    "\n",
    "Welcome to **Lesson 1** of the *DeepLearning.AI NLP Specialization* teaching notebook.  \n",
    "This notebook shows you **how to clean and prepare raw text** so it can be fed to classical machine‚Äëlearning algorithms *and* modern neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Why does text preprocessing matter? ü§î\n",
    "* Raw text is messy ‚Äì full of punctuation, contractions, emojis, hyperlinks and *noise* that distract a model.  \n",
    "* Careful preprocessing lets the model focus on **signal**, improving accuracy, reducing vocabulary size and training time.\n",
    "\n",
    "---\n",
    "\n",
    "## What you'll do in this notebook  üó∫Ô∏è\n",
    "\n",
    "1. **Build intuition with a toy example** ‚Äì walk a single sentence through tokenization ‚û°Ô∏è stop‚Äëword filtering ‚û°Ô∏è stemming.  \n",
    "2. **Peek at a real dataset** ‚Äì load the NLTK Twitter corpus and inspect its structure.  \n",
    "3. **Implement a complete preprocessing pipeline** you can reuse in later lessons and assignments.  \n",
    "4. **Play in an interactive Gradio demo** ‚Äì paste any tweet and watch each preprocessing step in real‚Äëtime.\n",
    "\n",
    "> üëâ Feel free to run each cell, tweak the code and break things. That's the fastest way to learn!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8abe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install & import required libraries\n",
    "# Colab automatically skips installations that already exist\n",
    "!pip -q install nltk==3.8.1 gradio==4.16.0 --progress-bar off\n",
    "\n",
    "import nltk, re, string, random\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download small NLTK resources üì•\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa4a3e",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Toy example ‚Äì preprocessing one sentence\n",
    "\n",
    "Before diving into thousands of tweets, let's warm‚Äëup with **one short sentence**.  \n",
    "We'll walk through each preprocessing step so you can *see* what changes and *why* it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love Natural Language Processing! :) #NLP\"\n",
    "\n",
    "print(\"üî∏ Original sentence:\")\n",
    "print(sentence)\n",
    "\n",
    "# 1. Tokenize\n",
    "tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                           strip_handles=True,\n",
    "                           reduce_len=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"\\nüîπ After tokenization:\")\n",
    "print(tokens)\n",
    "\n",
    "# 2. Remove stop‚Äëwords & punctuation\n",
    "stopwords_en = stopwords.words('english')\n",
    "tokens_no_sw = [w for w in tokens if w not in stopwords_en and w not in string.punctuation]\n",
    "print(\"\\nüîπ After stop‚Äëword & punctuation filtering:\")\n",
    "print(tokens_no_sw)\n",
    "\n",
    "# 3. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in tokens_no_sw]\n",
    "print(\"\\nüîπ After stemming:\")\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d35938",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "* `TweetTokenizer` **standardises** the text ‚Äì lower‚Äëcasing, normalising elongated words *soooo ‚Üí so*, and removing Twitter handles.  \n",
    "* Stop‚Äëwords like *'i', 'love'* (which carry little meaning for sentiment analysis) are filtered out.  \n",
    "* Finally, **stemming** reduces words to a common root (*processing ‚Üí process*), shrinking the vocabulary your model has to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a24bc",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Exploring the Twitter sentiment dataset\n",
    "\n",
    "We'll use the **`twitter_samples`** corpus that ships with NLTK.  \n",
    "It contains **5,000 positive** and **5,000 negative** English tweets originally collected by researchers at the University of Michigan.\n",
    "\n",
    "Let's load the text and take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ad3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive & negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_positive_tweets):,} positive tweets\")\n",
    "print(f\"‚úÖ Loaded {len(all_negative_tweets):,} negative tweets\")\n",
    "\n",
    "# Quick sanity check ‚Äì view one random tweet from each class\n",
    "print(\"\\nüî∏ Example positive tweet:\")\n",
    "print(random.choice(all_positive_tweets))\n",
    "print(\"\\nüî∏ Example negative tweet:\")\n",
    "print(random.choice(all_negative_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff307e13",
   "metadata": {},
   "source": [
    "*Notice the hashtags, emojis, URLs and random user mentions.*  \n",
    "We'll need to clean all that up before feeding the text to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5142e16",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Building a reusable `process_tweet()` function\n",
    "\n",
    "Instead of repeating the three steps above every time, let's wrap them in a **single helper** you can call from future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet: str):\n",
    "    \"\"\"Preprocess a single tweet into a list of cleaned, stemmed tokens.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Lower‚Äëcase and remove hyperlinks, hashtags & handles.\n",
    "    2. Tokenize with `TweetTokenizer`.\n",
    "    3. Filter stop‚Äëwords & punctuation.\n",
    "    4. Stem remaining tokens.\n",
    "    \"\"\"\n",
    "    # 1. Normalise text\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', tweet)   # remove links\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)                     # remove handles\n",
    "    tweet = re.sub(r'#', '', tweet)                         # strip hashtag symbol\n",
    "\n",
    "    # 2. Tokenise\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # 3. Remove stopwords & punctuation\n",
    "    stopwords_en = stopwords.words('english')\n",
    "    tokens_clean = [tok for tok in tokens\n",
    "                    if tok not in stopwords_en\n",
    "                    and tok not in string.punctuation]\n",
    "\n",
    "    # 4. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(tok) for tok in tokens_clean]\n",
    "\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = all_positive_tweets[2277]\n",
    "print(\"Original tweet:\\n\", sample)\n",
    "\n",
    "print(\"\\nProcessed tokens:\")\n",
    "print(process_tweet(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb63df5",
   "metadata": {},
   "source": [
    "‚ú® *Much cleaner!* You can now vectorise these tokens (e.g., with TF‚ÄëIDF or word embeddings) and train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89956",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Interactive playground üîß\n",
    "\n",
    "Use the widget below to **experiment** ‚Äì type any short snippet and see exactly what `process_tweet()` returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Helper for the demo\"\"\"\n",
    "    return ' '.join(process_tweet(text))\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=preprocess,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type a tweet here...\"),\n",
    "    outputs=gr.Textbox(label=\"Processed tokens\"),\n",
    "    title=\"Tweet Pre‚Äëprocessor\",\n",
    "    description=\"Watch how raw text is cleaned, tokenised and stemmed.\"\n",
    ")\n",
    "\n",
    "demo.launch(debug=False, share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085cc8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéâ You finished Lesson‚ÄØL1!\n",
    "\n",
    "You're now equipped to **clean tweets** for sentiment analysis and beyond.  \n",
    "In the next lesson you'll build features from these tokens and train your first classifier."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
