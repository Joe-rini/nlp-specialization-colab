{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e43335",
   "metadata": {},
   "source": [
    "# Lesson‚ÄØL2‚ÄØ‚Äì‚ÄØVisualizing‚ÄØWord‚ÄØFrequencies¬†üîçüìä  \n",
    "\n",
    "Welcome¬†to¬†**Lesson‚ÄØL2** of the DeepLearning.AI NLP Specialization remake!  \n",
    "\n",
    "## What¬†you‚Äôll¬†learn  \n",
    "* **Build** a frequency dictionary from text  \n",
    "* **Interpret** what ‚Äúword frequency‚Äù tells us about a corpus  \n",
    "* **Visualize** frequencies with bar charts & word¬†clouds  \n",
    "* **Explore** your own texts interactively with a Gradio app  \n",
    "\n",
    "## Why¬†does¬†this¬†matter?  \n",
    "Word frequencies are the *hello¬†world* of NLP analysis.  \n",
    "They underpin language modelling, sentiment analysis, TF‚ÄëIDF, and more.  \n",
    "Before diving into complex models, you should be comfortable asking:  \n",
    "\n",
    "> *‚ÄúWhich words appear most, and why?‚Äù*  \n",
    "\n",
    "## Roadmap  \n",
    "1. **Setup & installs** ‚Äì everything runs in Colab with one click  \n",
    "2. **Toy example** ‚Äì three tiny sentences to build intuition  \n",
    "3. **Real dataset** ‚Äì 10‚ÄØk tweets from NLTK  \n",
    "4. **Visualisations** ‚Äì bar plots and word¬†clouds  \n",
    "5. **Gradio playground** ‚Äì experiment live with your own text  \n",
    "\n",
    "_üëâ Let‚Äôs get started!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b61243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçÄ Colab setup ‚Äì run this first!\n",
    "!pip -q install nltk wordcloud gradio==4.12.0 --progress-bar off\n",
    "\n",
    "import nltk, ssl, os\n",
    "# Ensure SSL context for nltk downloader (occasional Colab issue)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Download required NLTK resources\n",
    "for resource in ['stopwords', 'punkt', 'twitter_samples']:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "# üîß¬†Patch for some NLTK versions that look for 'punkt_tab'\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('‚úÖ¬†NLTK data ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28310d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£¬†Toy¬†example ‚Äì build intuition  \n",
    "\n",
    "We‚Äôll start with **three micro‚Äësentences** so you can see every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "toy_sentences = [\n",
    "    \"I love NLP!\",\n",
    "    \"NLP loves Python.\",\n",
    "    \"Python loves you.\"\n",
    "]\n",
    "\n",
    "# Tokenise & lowercase\n",
    "tokens = [w.lower() for s in toy_sentences for w in word_tokenize(s)]\n",
    "freq = Counter(tokens)\n",
    "\n",
    "print(\"Frequencies:\", freq)\n",
    "\n",
    "# üìä bar plot\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(freq.keys(), freq.values())\n",
    "plt.title(\"Toy example word frequencies\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91d6c0",
   "metadata": {},
   "source": [
    "**What happened?**  \n",
    "* We tokenised sentences with `word_tokenize`.  \n",
    "* Lower‚Äëcased everything so ‚ÄúPython‚Äù and ‚Äúpython‚Äù merge.  \n",
    "* Counted with `Counter`.  \n",
    "* Visualised counts.  \n",
    "\n",
    "With just three lines you already have a **corpus fingerprint**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580d760",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£¬†Utility¬†functions (inline ‚Äì no¬†`utils.py`)  \n",
    "\n",
    "We now copy the helper code directly into the notebook so learners **don‚Äôt need extra files**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7333d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_tweet(tweet: str):\n",
    "    \"\"\"Clean, tokenize, remove stop‚Äëwords & stem.\"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?://\\S+', '', tweet)          # remove urls\n",
    "    tweet = re.sub(r'[^a-z\\s]', '', tweet)              # keep letters & space\n",
    "    tokens = [stemmer.stem(w) for w in tweet.split() if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build (word, label) -> freq dict.\"\"\"\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21dfce1",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£¬†Real‚Äëworld dataset ‚Äì 10‚ÄØk tweets  \n",
    "\n",
    "We‚Äôll reuse NLTK‚Äôs **twitter_samples** (5‚ÄØk positive + 5‚ÄØk negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "import numpy as np\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tweets = pos_tweets + neg_tweets\n",
    "ys = np.append(np.ones(len(pos_tweets)), np.zeros(len(neg_tweets)))\n",
    "\n",
    "print(f\"{len(tweets)} tweets loaded\")\n",
    "\n",
    "freqs = build_freqs(tweets, ys)\n",
    "print(f\"{len(freqs)} (word, label) pairs in frequency dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84556612",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£¬†Visualise top words  \n",
    "\n",
    "We‚Äôll plot the **20 most common words** irrespective of sentiment,  \n",
    "then generate separate **word clouds** for positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Aggregate counts ignoring sentiment\n",
    "from collections import Counter\n",
    "all_words = Counter()\n",
    "for (w, y), c in freqs.items():\n",
    "    all_words[w] += c\n",
    "\n",
    "top20 = all_words.most_common(20)\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(10,5))\n",
    "words, counts = zip(*top20)\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 20 words in the tweet corpus\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Word clouds\n",
    "pos_words = {w:c for (w,y),c in freqs.items() if y==1}\n",
    "neg_words = {w:c for (w,y),c in freqs.items() if y==0}\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,6))\n",
    "ax[0].imshow(WordCloud(width=400, height=300).generate_from_frequencies(pos_words))\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Positive tweets')\n",
    "\n",
    "ax[1].imshow(WordCloud(width=400, height=300).generate_from_frequencies(neg_words))\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Negative tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30e0e3",
   "metadata": {},
   "source": [
    "**Interpretation tips:**  \n",
    "* Common neutral tokens (e.g., ‚Äúlove‚Äù) may appear in both clouds.  \n",
    "* Compare *relative* prominence, not absolute counts.  \n",
    "* Notice slang or emojis that differ by sentiment ‚Äì an early cue for building classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d063e7",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£¬†Interactive Gradio playground¬†üéõÔ∏è  \n",
    "\n",
    "Paste any text and instantly see its word frequencies.  \n",
    "Try lyrics, news headlines, or even your own tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from collections import Counter\n",
    "\n",
    "def freq_dict(text):\n",
    "    tokens = process_tweet(text)\n",
    "    return dict(Counter(tokens))\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"### üîé Word‚Äëfrequency explorer  \n",
    "Enter text, press **Run**, and inspect the resulting dictionary.\"\"\")\n",
    "    inp = gr.Textbox(label=\"Your text\", lines=4)\n",
    "    out = gr.JSON(label=\"Frequencies\")\n",
    "    inp.submit(freq_dict, inp, out)\n",
    "    gr.Button(\"Run\").click(freq_dict, inp, out)\n",
    "\n",
    "# Uncomment the line below when running interactively in Colab\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b9015",
   "metadata": {},
   "source": [
    "---  \n",
    "üéâ **You‚Äôve visualised word frequencies from scratch!**  \n",
    "Feel free to tweak the code and explore further.  \n",
    "Next lesson: we‚Äôll build a **Logistic Regression classifier** using these frequencies as features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
