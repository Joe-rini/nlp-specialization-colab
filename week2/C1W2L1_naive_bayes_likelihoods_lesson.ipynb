{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dba6c4e",
   "metadata": {},
   "source": [
    "# ğŸ“˜ C1W2_L1: Naive Bayes Likelihoods â€“ Teaching Version\n",
    "\n",
    "ğŸ¯ **Let's build a classifier. First Toy, then real!**\n",
    "\n",
    "In this workbook, our goal is to build a system that can predict whether a Tweet is positive or negative. Like last weekâ€™s logistic regression, weâ€™ll keep things simple and interpretable: no word order, no phrasing â€” just plain counts of which words appear.\n",
    "\n",
    "But instead of learning weights through optimization (like logistic regression), this time weâ€™ll build a model using **Bayesâ€™ Theorem** and **word frequencies**. Think of this as letting probabilities do the talking, based on how often a word appears in positive vs. negative tweets.\n",
    "\n",
    "Youâ€™ll see lots of overlap with Week 1â€™s bag-of-words approach â€” but the core *math* behind the prediction is different. Letâ€™s dive in!\n",
    "\n",
    "Unlike logistic regression last week, thereâ€™s no optimization or gradient descent here â€” just a lookup-based statistical model derived directly from the labeled data. Itâ€™s still â€œlearning,â€ but itâ€™s more like rule-building from observed frequencie\n",
    "ğŸ§  So is this â€œlearningâ€?\n",
    "Barely. In the broadest ML sense: yes, because the model's behavior changes depending on what data it sees.\n",
    "\n",
    "But:\n",
    "\n",
    "Itâ€™s parametric (small number of parameters: priors and word likelihoods)\n",
    "\n",
    "Itâ€™s closed-form (you just calculate values)\n",
    "\n",
    "Itâ€™s non-adaptive beyond word frequencies\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What is Naive Bayes?\n",
    "Naive Bayes is a simple yet powerful classification algorithm based on **Bayesâ€™ Theorem**:\n",
    "\n",
    "\\[ P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)} \\]\n",
    "\n",
    "In our case:\n",
    "- A = a sentiment label (e.g., positive or negative)\n",
    "- B = the tweetâ€™s words\n",
    "\n",
    "The â€œnaiveâ€ assumption is that all words in a tweet are **conditionally independent** given the sentiment label.\n",
    "\n",
    "This means we treat \"bad\" and \"dream\" as unrelated â€” even though \"bad dream\" might have a specific negative meaning together. Thatâ€™s the simplification â€” and limitation â€” of this approach.\n",
    "\n",
    "The classifier works by computing a **log-likelihood score** for each label:\n",
    "\n",
    "\\[ \\text{score} = \\log(P(\\text{label})) + \\sum_{i} \\log(P(\\text{word}_i \\mid \\text{label})) \\]\n",
    "\n",
    "We choose the label with the highest score. This is different from last week, where the score came from a **sigmoid curve fitted by gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” How is this similar to last week?\n",
    "- âœ… We still represent tweets using **bag-of-words** (no order, just counts)\n",
    "- âœ… We still want to predict positive vs negative\n",
    "- âœ… We still calculate a score per tweet\n",
    "\n",
    "## ğŸ” Whatâ€™s different this week?\n",
    "- âŒ No learned weights from gradient descent\n",
    "- âœ… We calculate probabilities directly from word frequencies\n",
    "- âœ… We use **log-likelihoods** instead of a sigmoid decision function\n",
    "\n",
    "In short, both methods try to split tweets by some kind of score â€” but how that score is **calculated** is different.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ 1. Setup & Downloads\n",
    "We'll begin by installing dependencies and loading some Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba8b58",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "C1W2_L1_Naive_Bayes_Teaching.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
