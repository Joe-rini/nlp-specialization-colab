{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca9b691",
   "metadata": {},
   "source": [
    "# 📘 C1W2_L1: Naive Bayes Likelihoods – Teaching Version\n",
    "\n",
    "Welcome! In this lesson, you'll explore how Naive Bayes classification works under the hood, focusing on how word frequencies affect predictions. You’ll learn to:\n",
    "\n",
    "- Process tweets and create a frequency dictionary\n",
    "- Train a Naive Bayes model and compute log-likelihoods\n",
    "- Visualize the most informative words\n",
    "- Build an interactive sentiment classifier with Gradio\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 1. Setup & Downloads\n",
    "We'll begin by installing dependencies and loading some Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b225b3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📥 2. Imports\n",
    "Here are the packages we’ll use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gradio as gr\n",
    "\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9e0cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💬 3. Load and Peek at the Data\n",
    "We’ll use 5,000 positive and 5,000 negative tweets from NLTK’s corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66073b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(\"Sample positive tweet:\")\n",
    "print(all_positive_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889a081",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧹 4. Preprocess the Tweets\n",
    "This function:\n",
    "- Tokenizes tweets\n",
    "- Removes stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a798b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    clean = [word for word in tokens if word not in stopwords_english and word not in string.punctuation]\n",
    "    return clean\n",
    "\n",
    "# Try it on a tweet:\n",
    "process_tweet(all_negative_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd4e4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 5. Create Frequency Dictionary\n",
    "We now build a dictionary with (word, label) → count pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8456a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(freq_dict, tweets, labels):\n",
    "    for label, tweet in zip(labels, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, label)\n",
    "            freq_dict[pair] = freq_dict.get(pair, 0) + 1\n",
    "    return freq_dict\n",
    "\n",
    "# Small test:\n",
    "test_freq = count_tweets({}, [\"I am happy\", \"I am sad\"], [1, 0])\n",
    "print(test_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c0fd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📐 6. Train Naive Bayes\n",
    "We compute:\n",
    "- `logprior` from class distribution\n",
    "- `loglikelihoods` for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs:\n",
    "        if pair[1] == 1:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    D = len(train_y)\n",
    "    D_pos = sum(train_y)\n",
    "    D_neg = D - D_pos\n",
    "    logprior = np.log(D_pos / D_neg)\n",
    "    for word in vocab:\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e04a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏗️ 7. Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = all_positive_tweets[:4000] + all_negative_tweets[:4000]\n",
    "train_y = np.append(np.ones(4000), np.zeros(4000))\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d982b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔮 8. Make Predictions\n",
    "Here’s the prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    words = process_tweet(tweet)\n",
    "    score = logprior\n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            score += loglikelihood[word]\n",
    "    return score\n",
    "\n",
    "# Test example:\n",
    "tweet = \"Today is awesome!\"\n",
    "print(f\"Score: {naive_bayes_predict(tweet, logprior, loglikelihood):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7342c12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 9. Visualize Top Words\n",
    "Let’s see which words are most influential in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1826a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglikelihoods(loglikelihood):\n",
    "    top_words = sorted(loglikelihood.items(), key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "    words, vals = zip(*top_words)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(vals), y=list(words))\n",
    "    plt.title(\"Most Influential Words\")\n",
    "    plt.xlabel(\"Log-likelihood\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loglikelihoods(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a47f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 10. Interactive Sentiment Classifier\n",
    "Use Gradio to test tweets live!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    score = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    label = \"Positive 😀\" if score > 0 else \"Negative 😞\"\n",
    "    return f\"Score: {score:.2f} → {label}\"\n",
    "\n",
    "demo = gr.Interface(fn=classify_tweet,\n",
    "                    inputs=gr.Textbox(lines=2, placeholder=\"Enter a tweet...\"),\n",
    "                    outputs=\"text\",\n",
    "                    title=\"Naive Bayes Tweet Sentiment Classifier\")\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabc233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "In this lesson, you:\n",
    "- Learned how to compute log-likelihoods from tweet data\n",
    "- Built a Naive Bayes sentiment classifier from scratch\n",
    "- Visualized word influence\n",
    "- Created a working interactive classifier\n",
    "\n",
    "Nice work!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "C1W2_L1_Naive_Bayes_Teaching.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
