{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dba6c4e",
      "metadata": {
        "id": "8dba6c4e"
      },
      "source": [
        "# üìò C1W2_L1: Naive Bayes Likelihoods ‚Äì Teaching Version\n",
        "\n",
        "In this workbook, our goal is to build a system that can predict whether a Tweet is positive or negative using the Naive Bayes approach. Like last week‚Äôs logistic regression, we‚Äôll keep things simple and interpretable: no word order, no phrasing ‚Äî just plain counts of which words appear.\n",
        "\n",
        "But instead of learning weights through optimization (like logistic regression), this time we‚Äôll build a model using **Bayes‚Äô Theorem** and **word frequencies**. Think of this as letting probabilities do the talking, based on how often a word appears in positive vs. negative tweets.\n",
        "\n",
        "You‚Äôll see lots of overlap with Week 1‚Äôs bag-of-words approach ‚Äî but the core *math* behind the prediction is different. Let‚Äôs dive in!\n",
        "\n",
        "Unlike logistic regression last week, there‚Äôs no optimization or gradient descent here ‚Äî just a lookup-based statistical model derived directly from the labeled data.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is Naive Bayes?\n",
        "Naive Bayes is a simple yet powerful classification algorithm based on **Bayes‚Äô Theorem**:\n",
        "\n",
        "The ‚Äúnaive‚Äù assumption is that all words in a tweet are **conditionally independent** given the sentiment label (whether the Tweet is Positive or Negative).\n",
        "\n",
        "üßÆ How Naive Bayes Works (Simple Steps):\n",
        "\n",
        "1Ô∏è‚É£ Clean and tokenize each tweet (remove stopwords, punctuation, etc.)\n",
        "\n",
        "2Ô∏è‚É£ Count how often each word appears in positive and negative tweets\n",
        "\n",
        "3Ô∏è‚É£ Calculate smoothed probabilities\n",
        "\n",
        "4Ô∏è‚É£ For a new tweet, sum the log probabilities of each word under each label\n",
        "\n",
        "5Ô∏è‚É£ Predict the label with the higher total score\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## üîß 1. Setup & Downloads\n",
        "We'll begin by installing dependencies and loading some Twitter data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e27f2e6",
      "metadata": {
        "id": "3e27f2e6"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ba8b58",
      "metadata": {
        "id": "36ba8b58"
      },
      "source": [
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C1W2_L1_Naive_Bayes_Teaching.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}