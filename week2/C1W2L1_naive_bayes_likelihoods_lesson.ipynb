{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dba6c4e",
      "metadata": {
        "id": "8dba6c4e"
      },
      "source": [
        "# 📘 C1W2_L1: Naive Bayes Likelihoods – Teaching Version\n",
        "\n",
        "In this workbook, our goal is to build a system that can predict whether a Tweet is positive or negative using the Naive Bayes approach. Like last week’s logistic regression, we’ll keep things simple and interpretable: no word order, no phrasing — just plain counts of which words appear.\n",
        "\n",
        "But instead of learning weights through optimization (like logistic regression), this time we’ll build a model using **Bayes’ Theorem** and **word frequencies**. Think of this as letting probabilities do the talking, based on how often a word appears in positive vs. negative tweets.\n",
        "\n",
        "You’ll see lots of overlap with Week 1’s bag-of-words approach — but the core *math* behind the prediction is different. Let’s dive in!\n",
        "\n",
        "Unlike logistic regression last week, there’s no optimization or gradient descent here — just a lookup-based statistical model derived directly from the labeled data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What is Naive Bayes?\n",
        "Naive Bayes is a simple yet powerful classification algorithm based on **Bayes’ Theorem**:\n",
        "\n",
        "The “naive” assumption is that all words in a tweet are **conditionally independent** given the sentiment label (whether the Tweet is Positive or Negative).\n",
        "\n",
        "🧮 How Naive Bayes Works (Simple Steps):\n",
        "\n",
        "1️⃣ Clean and tokenize each tweet (remove stopwords, punctuation, etc.)\n",
        "\n",
        "2️⃣ Count how often each word appears in positive and negative tweets\n",
        "\n",
        "3️⃣ Calculate smoothed probabilities\n",
        "\n",
        "4️⃣ For a new tweet, sum the log probabilities of each word under each label\n",
        "\n",
        "5️⃣ Predict the label with the higher total score\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
         "## 🔧 1. Setup & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbe7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322cc73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📥 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gradio as gr\n",
    "\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ffbe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💬 3. Load and Peek at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(\"Sample positive tweet:\")\n",
    "print(all_positive_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1bca2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧹 4. Preprocess the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    clean = [word for word in tokens if word not in stopwords_english and word not in string.punctuation]\n",
    "    return clean\n",
    "\n",
    "# Try on a sample\n",
    "process_tweet(all_negative_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723ffa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 5. Count Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce52b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(freq_dict, tweets, labels):\n",
    "    for label, tweet in zip(labels, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, label)\n",
    "            freq_dict[pair] = freq_dict.get(pair, 0) + 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2781d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📐 6. Train Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs:\n",
    "        if pair[1] == 1:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    D = len(train_y)\n",
    "    D_pos = sum(train_y)\n",
    "    D_neg = D - D_pos\n",
    "    logprior = np.log(D_pos / D_neg)\n",
    "    for word in vocab:\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99b1a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏗️ 7. Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78131ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = all_positive_tweets[:4000] + all_negative_tweets[:4000]\n",
    "train_y = np.append(np.ones(4000), np.zeros(4000))\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7a1ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔮 8. Predict on New Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd63969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    words = process_tweet(tweet)\n",
    "    score = logprior\n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            score += loglikelihood[word]\n",
    "    return score\n",
    "\n",
    "# Try a test tweet\n",
    "tweet = \"Today is awesome!\"\n",
    "print(f\"Score: {naive_bayes_predict(tweet, logprior, loglikelihood):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f9422",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 9. Visualize Influential Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd732c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglikelihoods(loglikelihood):\n",
    "    top_words = sorted(loglikelihood.items(), key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "    words, vals = zip(*top_words)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(vals), y=list(words))\n",
    "    plt.title(\"Most Influential Words\")\n",
    "    plt.xlabel(\"Log-likelihood\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loglikelihoods(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4119a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 10. Gradio Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    score = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    label = \"Positive 😀\" if score > 0 else \"Negative 😞\"\n",
    "    return f\"Score: {score:.2f} → {label}\"\n",
    "\n",
    "demo = gr.Interface(fn=classify_tweet,\n",
    "                    inputs=gr.Textbox(lines=2, placeholder=\"Enter a tweet...\"),\n",
    "                    outputs=\"text\",\n",
    "                    title=\"Naive Bayes Tweet Sentiment Classifier\")\n",
    "\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "C1W2_L1_Naive_Bayes_Teaching.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
