{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dba6c4e",
      "metadata": {
        "id": "8dba6c4e"
      },
      "source": [
        "# ðŸ“˜ C1W2_L1: Naive Bayes Likelihoods â€“ Teaching Version\n",
        "\n",
        "In this workbook, our goal is to build a system that can predict whether a Tweet is positive or negative using the Naive Bayes approach. Like last weekâ€™s logistic regression, weâ€™ll keep things simple and interpretable: no word order, no phrasing â€” just plain counts of which words appear.\n",
        "\n",
        "But instead of learning weights through optimization (like logistic regression), this time weâ€™ll build a model using **Bayesâ€™ Theorem** and **word frequencies**. Think of this as letting probabilities do the talking, based on how often a word appears in positive vs. negative tweets.\n",
        "\n",
        "Youâ€™ll see lots of overlap with Week 1â€™s bag-of-words approach â€” but the core *math* behind the prediction is different. Letâ€™s dive in!\n",
        "\n",
        "Unlike logistic regression last week, thereâ€™s no optimization or gradient descent here â€” just a lookup-based statistical model derived directly from the labeled data.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  What is Naive Bayes?\n",
        "Naive Bayes is a simple yet powerful classification algorithm based on **Bayesâ€™ Theorem**:\n",
        "\n",
        "The â€œnaiveâ€ assumption is that all words in a tweet are **conditionally independent** given the sentiment label (whether the Tweet is Positive or Negative).\n",
        "\n",
        "ðŸ§® How Naive Bayes Works (Simple Steps):\n",
        "\n",
        "1ï¸âƒ£ Clean and tokenize each tweet (remove stopwords, punctuation, etc.)\n",
        "\n",
        "2ï¸âƒ£ Count how often each word appears in positive and negative tweets\n",
        "\n",
        "3ï¸âƒ£ Calculate smoothed probabilities\n",
        "\n",
        "4ï¸âƒ£ For a new tweet, sum the log probabilities of each word under each label\n",
        "\n",
        "5ï¸âƒ£ Predict the label with the higher total score\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
         "## ðŸ”§ 1. Setup & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbe7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322cc73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¥ 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gradio as gr\n",
    "\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ffbe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ’¬ 3. Load and Peek at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(\"Sample positive tweet:\")\n",
    "print(all_positive_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1bca2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§¹ 4. Preprocess the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    clean = [word for word in tokens if word not in stopwords_english and word not in string.punctuation]\n",
    "    return clean\n",
    "\n",
    "# Try on a sample\n",
    "process_tweet(all_negative_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723ffa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š 5. Count Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce52b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(freq_dict, tweets, labels):\n",
    "    for label, tweet in zip(labels, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, label)\n",
    "            freq_dict[pair] = freq_dict.get(pair, 0) + 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2781d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ 6. Train Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs:\n",
    "        if pair[1] == 1:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    D = len(train_y)\n",
    "    D_pos = sum(train_y)\n",
    "    D_neg = D - D_pos\n",
    "    logprior = np.log(D_pos / D_neg)\n",
    "    for word in vocab:\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99b1a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ 7. Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78131ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = all_positive_tweets[:4000] + all_negative_tweets[:4000]\n",
    "train_y = np.append(np.ones(4000), np.zeros(4000))\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7a1ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”® 8. Predict on New Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd63969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    words = process_tweet(tweet)\n",
    "    score = logprior\n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            score += loglikelihood[word]\n",
    "    return score\n",
    "\n",
    "# Try a test tweet\n",
    "tweet = \"Today is awesome!\"\n",
    "print(f\"Score: {naive_bayes_predict(tweet, logprior, loglikelihood):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f9422",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š 9. Visualize Influential Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd732c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglikelihoods(loglikelihood):\n",
    "    top_words = sorted(loglikelihood.items(), key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "    words, vals = zip(*top_words)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=list(vals), y=list(words))\n",
    "    plt.title(\"Most Influential Words\")\n",
    "    plt.xlabel(\"Log-likelihood\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loglikelihoods(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4119a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ 10. Gradio Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    score = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    label = \"Positive ðŸ˜€\" if score > 0 else \"Negative ðŸ˜ž\"\n",
    "    return f\"Score: {score:.2f} â†’ {label}\"\n",
    "\n",
    "demo = gr.Interface(fn=classify_tweet,\n",
    "                    inputs=gr.Textbox(lines=2, placeholder=\"Enter a tweet...\"),\n",
    "                    outputs=\"text\",\n",
    "                    title=\"Naive Bayes Tweet Sentiment Classifier\")\n",
    "\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "C1W2_L1_Naive_Bayes_Teaching.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
